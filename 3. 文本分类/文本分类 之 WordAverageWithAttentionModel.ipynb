{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word average + attention\n",
    "我们现在定义一种基于简单attention机制的编码器(encoder)。这种编码器针对句子中的每个单词生成一个权重，然后句子中每个单词的词向量的加权平均就用来表示这个句子:\n",
    "$$\\alpha_t \\propto exp\\{cos(u, emb(x_t))\\}$$\n",
    "$$h_{att} = \\sum_t a_t*emb(x_t) $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后句子的正面情感概率就用下面的式子计算：\n",
    "$$\\sigma(W^T * h_{att})$$\n",
    "\n",
    "\\sigma是逻辑斯蒂函数，w是一个d维向量，作为模型的参数。\n",
    "\n",
    "attention模型与word average模型的不同之处在于我们增加了一个d维向量参数 u 用于计算单词的权重，也是模型参数的一部分。\n",
    "\n",
    "在这个模型中，我们用u和单词embedding的cosine similarity表示单词的重要性。我们会用一层softmax来把这些权重归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hide new secretions from the parental units\\t0\\n', 'contains no wit , only labored gags\\t0\\n', 'that loves its characters and communicates something rather beautiful about human nature\\t1\\n', 'remains utterly satisfied to remain the same throughout\\t0\\n', 'on the worst revenge-of-the-nerds clich茅s the filmmakers could dredge up\\t0\\n', \"that 's far too tragic to merit such superficial treatment\\t0\\n\", 'demonstrates that the director of such Hollywood blockbusters as Patriot Games can still turn out a small , personal film with an emotional wallop .\\t1\\n', 'of saucy\\t1\\n', \"a depressed fifteen-year-old 's suicidal poetry\\t0\\n\", \"are more deeply thought through than in most ` right-thinking ' films\\t1\\n\"]\n"
     ]
    }
   ],
   "source": [
    "with open('senti.train.tsv','r') as rf:\n",
    "    lines = rf.readlines()\n",
    "print(lines[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(path,'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            sentence, label = line.split('\\t')\n",
    "            sentences.append(sentence.lower().split())\n",
    "            labels.append(label[0])\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path,dev_path,test_path = 'senti.train.tsv','senti.dev.tsv','senti.test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences, train_labels = read_corpus(train_path)\n",
    "dev_sentences, dev_labels = read_corpus(dev_path)\n",
    "test_sentences, test_labels = read_corpus(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67349\n",
      "67349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_sentences)), print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['contains', 'no', 'wit', ',', 'only', 'labored', 'gags'], '0')"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[1], train_labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(sentences, word_size=20000):\n",
    "    c = Counter()\n",
    "    for sent in sentences:\n",
    "        for word in sent:\n",
    "            c[word] += 1\n",
    "    print('文本总单词量为：',len(c))\n",
    "    words_most_common = c.most_common(word_size)\n",
    "    ## adding unk, pad\n",
    "    idx2word = ['<pad>','<unk>'] + [item[0] for item in words_most_common]\n",
    "    word2dix = {w:i for i, w in enumerate(idx2word)}\n",
    "    return idx2word, word2dix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本总单词量为： 14828\n"
     ]
    }
   ],
   "source": [
    "WORD_SIZE=20000\n",
    "idx2word, word2dix = build_vocab(train_sentences, word_size=WORD_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<unk>', 'the', ',', 'a', 'and', 'of', '.', 'to', \"'s\"]"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numeralization(sentences, labels, word2idx):\n",
    "    '把word list表示的句子转成 index 表示的列表'\n",
    "    numeral_sent = [[word2dix.get(w, word2dix['<unk>']) for w in s] for s in sentences]\n",
    "    numeral_label =[int(label) for label in labels]\n",
    "    return list(zip(numeral_sent, numeral_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train_data = numeralization(train_sentences, train_labels, word2dix)\n",
    "num_test_data = numeralization(test_sentences, test_labels, word2dix)\n",
    "num_dev_data = numeralization(dev_sentences, dev_labels, word2dix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert2tensor(batch_sentences):\n",
    "    '将batch数据转成tensor,这里主要是为了padding 补全0'\n",
    "    lengths = [len(s) for s in batch_sentences]\n",
    "    max_len = max(lengths)\n",
    "    batch_size = len(batch_sentences)\n",
    "    batch = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
    "    for i, l in enumerate(lengths):\n",
    "        batch[i, :l] = torch.tensor(batch_sentences[i])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(numeral_sentences_labels, batch_size=32):\n",
    "    '''将list index 数据 分成batch '''\n",
    "    batches = []\n",
    "    num_sample = len(numeral_sentences_labels)\n",
    "    random.shuffle(numeral_sentences_labels)\n",
    "    numeral_sent = [n[0] for n in numeral_sentences_labels]\n",
    "    numeral_label = [n[1] for n in numeral_sentences_labels]\n",
    "    for start in range(0, num_sample, batch_size):\n",
    "        end = start + batch_size\n",
    "        if end > num_sample:\n",
    "            batch_sentences = numeral_sent[start : num_sample]\n",
    "            batch_labels = numeral_label[start : num_sample]\n",
    "            batch_sent_tensor = convert2tensor(batch_sentences)\n",
    "            batch_label_tensor = torch.tensor(batch_labels, dtype=torch.float)\n",
    "        else:\n",
    "            batch_sentences = numeral_sent[start : end]\n",
    "            batch_labels = numeral_label[start : end]\n",
    "            batch_sent_tensor = convert2tensor(batch_sentences)\n",
    "            batch_label_tensor = torch.tensor(batch_labels, dtype=torch.float)\n",
    "        batches.append((batch_sent_tensor.cuda(), batch_label_tensor.cuda()))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = generate_batch(num_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AVGAttenModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, output_size, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.u = nn.Parameter(torch.randn(embed_dim))\n",
    "        self.fc = nn.Linear(embed_dim, output_size)\n",
    "    def forward(self, text):\n",
    "        ## [batch_size, seq_len]->[batch_size, seq_len, embed_dim]\n",
    "        embed = self.embedding(text)\n",
    "        ## attention\n",
    "        ## 扩展u [embed_dim] ->[batch_size, seq_len, embed_dim]\n",
    "        u = self.u.repeat(embed.size(0), embed.size(1), 1)\n",
    "        ## cos: [batch_size, seq_len] 每个位置都是 batch_size, seq_len 对应的cos值\n",
    "        cos = F.cosine_similarity(embed, u, dim=2) ## 沿着dim=2计算 \n",
    "        alpha = F.softmax(cos, dim=1) ## 求权重 batch_size, seq_len ##对一个句子内的cosj进行归一化\n",
    "        # [batch_size, embed_size] ## embed 乘以对应位置的权重\n",
    "        h_attn = torch.sum(embed * alpha.unsqueeze(2), dim=1).squeeze(1)\n",
    "        out = self.fc(h_attn)\n",
    "        return out\n",
    "    def get_embed_weigth(self):\n",
    "        return self.embedding.weight.data\n",
    "    def get_u(self):\n",
    "        return self.u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word2dix)\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_SIZE = 1\n",
    "PAD_IDX = word2dix['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AVGAttenModel(\n",
       "  (embedding): Embedding(14830, 100, padding_idx=0)\n",
       "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AVGAttenModel(vocab_size=VOCAB_SIZE,\n",
    "                 embed_dim=EMBEDDING_DIM,\n",
    "                 output_size=OUTPUT_SIZE, \n",
    "                 pad_idx=PAD_IDX)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数 和优化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(output, label):\n",
    "    ## output: batch_size \n",
    "    y_hat = torch.round(torch.sigmoid(output)) ## 将output 转成0和1\n",
    "    correct = (y_hat == label).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(batch_data, model, criterion, get_accuracy):\n",
    "    model.eval()\n",
    "    num_epoch = epoch_loss = epoch_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for text, label in batch_data:\n",
    "            out = model(text).squeeze(1)\n",
    "            loss = criterion(out, label)\n",
    "            acc = get_accuracy(out, label)\n",
    "            num_epoch +=1 \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss/num_epoch, epoch_acc/num_epoch          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(batch_data, model, criterion, optimizer, get_accuracy):\n",
    "    model.train()\n",
    "    num_epoch = epoch_loss = epoch_acc = 0\n",
    "    for text, label in batch_data:\n",
    "        model.zero_grad()\n",
    "        out = model(text).squeeze(1)\n",
    "        loss = criterion(out, label)\n",
    "        acc = get_accuracy(out, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        num_epoch +=1 \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss/num_epoch, epoch_acc/num_epoch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 :\n",
      "\t Train Loss: 0.2159 | Train Acc: 92.08%\n",
      "\t Valid Loss: 0.4092 | Valid Acc: 82.59%\n",
      "Epoch: 02 :\n",
      "\t Train Loss: 0.2047 | Train Acc: 92.44%\n",
      "\t Valid Loss: 0.4172 | Valid Acc: 82.70%\n",
      "Epoch: 03 :\n",
      "\t Train Loss: 0.1956 | Train Acc: 92.80%\n",
      "\t Valid Loss: 0.4296 | Valid Acc: 82.70%\n",
      "Epoch: 04 :\n",
      "\t Train Loss: 0.1872 | Train Acc: 93.02%\n",
      "\t Valid Loss: 0.4389 | Valid Acc: 82.81%\n",
      "Epoch: 05 :\n",
      "\t Train Loss: 0.1802 | Train Acc: 93.29%\n",
      "\t Valid Loss: 0.4473 | Valid Acc: 82.70%\n",
      "Epoch: 06 :\n",
      "\t Train Loss: 0.1740 | Train Acc: 93.51%\n",
      "\t Valid Loss: 0.4600 | Valid Acc: 82.59%\n",
      "Epoch: 07 :\n",
      "\t Train Loss: 0.1688 | Train Acc: 93.70%\n",
      "\t Valid Loss: 0.4731 | Valid Acc: 82.70%\n",
      "Epoch: 08 :\n",
      "\t Train Loss: 0.1640 | Train Acc: 93.93%\n",
      "\t Valid Loss: 0.4810 | Valid Acc: 82.81%\n",
      "Epoch: 09 :\n",
      "\t Train Loss: 0.1589 | Train Acc: 94.09%\n",
      "\t Valid Loss: 0.4955 | Valid Acc: 82.48%\n",
      "Epoch: 10 :\n",
      "\t Train Loss: 0.1559 | Train Acc: 94.19%\n",
      "\t Valid Loss: 0.5087 | Valid Acc: 82.48%\n",
      "Epoch: 11 :\n",
      "\t Train Loss: 0.1518 | Train Acc: 94.33%\n",
      "\t Valid Loss: 0.5186 | Valid Acc: 82.48%\n",
      "Epoch: 12 :\n",
      "\t Train Loss: 0.1489 | Train Acc: 94.45%\n",
      "\t Valid Loss: 0.5310 | Valid Acc: 82.37%\n",
      "Epoch: 13 :\n",
      "\t Train Loss: 0.1457 | Train Acc: 94.55%\n",
      "\t Valid Loss: 0.5434 | Valid Acc: 82.59%\n",
      "Epoch: 14 :\n",
      "\t Train Loss: 0.1431 | Train Acc: 94.66%\n",
      "\t Valid Loss: 0.5581 | Valid Acc: 82.14%\n",
      "Epoch: 15 :\n",
      "\t Train Loss: 0.1403 | Train Acc: 94.75%\n",
      "\t Valid Loss: 0.5686 | Valid Acc: 82.03%\n",
      "Epoch: 16 :\n",
      "\t Train Loss: 0.1378 | Train Acc: 94.84%\n",
      "\t Valid Loss: 0.5810 | Valid Acc: 82.03%\n",
      "Epoch: 17 :\n",
      "\t Train Loss: 0.1357 | Train Acc: 94.99%\n",
      "\t Valid Loss: 0.5932 | Valid Acc: 82.48%\n",
      "Epoch: 18 :\n",
      "\t Train Loss: 0.1337 | Train Acc: 95.02%\n",
      "\t Valid Loss: 0.6074 | Valid Acc: 82.14%\n",
      "Epoch: 19 :\n",
      "\t Train Loss: 0.1315 | Train Acc: 95.06%\n",
      "\t Valid Loss: 0.6212 | Valid Acc: 82.03%\n",
      "Epoch: 20 :\n",
      "\t Train Loss: 0.1302 | Train Acc: 95.13%\n",
      "\t Valid Loss: 0.6346 | Valid Acc: 81.81%\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCH = 20\n",
    "best_valid_acc = -1\n",
    "\n",
    "dev_data = generate_batch(num_dev_data)\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    train_data = generate_batch(num_train_data)\n",
    "    train_loss, train_acc = train(train_data, model, criterion, optimizer, get_accuracy)\n",
    "    valid_loss, valid_acc = evaluate(dev_data, model, criterion, get_accuracy)\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(model.state_dict(),'avg-atten-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} :')\n",
    "    print(f'\\t Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc*100:.2f}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('avg-atten-model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3854 |  Test Acc: 82.89%\n"
     ]
    }
   ],
   "source": [
    "test_data = generate_batch(num_test_data)\n",
    "test_loss, test_acc = evaluate(test_data, model, criterion, get_accuracy)\n",
    "print(f'Test Loss: {test_loss:.4f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打印词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "embed = model.get_embed_weigth() ##  vocab_size * embed_size\n",
    "u = model.get_u()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14830, 100])"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "u = u.repeat(embed.size(0), 1)\n",
    "with torch.no_grad():\n",
    "    cos = F.cosine_similarity(embed, u, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_socre, sorted_idx = cos.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6202, 0.6207, 0.6222, 0.6338, 0.6354, 0.6356, 0.6402, 0.6418, 0.6988,\n",
       "        0.7162], device='cuda:0')"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_socre[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention 最小的30个单词：\n",
      "the   -0.9967465400695801 \n",
      "<pad>   -0.9959186315536499 \n",
      "'s   -0.9858197569847107 \n",
      "a   -0.9832067489624023 \n",
      "for   -0.9813275337219238 \n",
      "is   -0.9744138121604919 \n",
      "your   -0.9691771864891052 \n",
      ",   -0.9663684964179993 \n",
      "very   -0.9609249830245972 \n",
      "directed   -0.9591320753097534 \n",
      "in   -0.9578869938850403 \n",
      "time   -0.9567098021507263 \n",
      ".   -0.9532570838928223 \n",
      "can   -0.9528145790100098 \n",
      "his   -0.9518136382102966 \n",
      "sense   -0.9483051300048828 \n",
      "of   -0.9468281269073486 \n",
      "story   -0.9467564821243286 \n",
      "to   -0.9453473687171936 \n",
      "--   -0.9451014399528503 \n",
      "it   -0.944945752620697 \n",
      "that   -0.9435582160949707 \n",
      "its   -0.9423429369926453 \n",
      "-lrb-   -0.938753068447113 \n",
      "about   -0.9368602633476257 \n",
      "two   -0.9318870902061462 \n",
      "are   -0.9273205995559692 \n",
      "such   -0.9272398948669434 \n",
      "words   -0.9214128851890564 \n",
      "''   -0.9189384579658508 \n"
     ]
    }
   ],
   "source": [
    "print('attention 最小的30个单词：')\n",
    "for i,s in zip(sorted_idx[:30], sorted_socre[:30]):\n",
    "    print(idx2word[i], ' ', s.item(), end=' \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atten 最大的30个单词：\n",
      "dim-witted   0.6067432761192322 \n",
      "slow   0.6074190139770508 \n",
      "oppressively   0.608052134513855 \n",
      "neither   0.6080911755561829 \n",
      "pointless   0.6082792282104492 \n",
      "saddest   0.608433723449707 \n",
      "mediocre   0.6101371049880981 \n",
      "drag   0.6104316115379333 \n",
      "tiresomely   0.6107774376869202 \n",
      "drab   0.6113846302032471 \n",
      "annoying   0.612328052520752 \n",
      "falls   0.6125514507293701 \n",
      "smug   0.6126384735107422 \n",
      "missed   0.6136465072631836 \n",
      "hack   0.6157366633415222 \n",
      "lacks   0.6180804371833801 \n",
      "horrible   0.6181895136833191 \n",
      "deadly   0.6193328499794006 \n",
      "off-putting   0.6193955540657043 \n",
      "clueless   0.6194455027580261 \n",
      "exit   0.6202219128608704 \n",
      "rolling   0.6207405924797058 \n",
      "misery   0.6222171187400818 \n",
      "inc.   0.633823812007904 \n",
      "not   0.6354070901870728 \n",
      "problem   0.6355876922607422 \n",
      "bad   0.6402461528778076 \n",
      "sanctimonious   0.6418283581733704 \n",
      "wrong   0.6987859606742859 \n",
      "trouble   0.7162488698959351 \n"
     ]
    }
   ],
   "source": [
    "print('atten 最大的30个单词：')\n",
    "for i,s in zip(sorted_idx[-30:], sorted_socre[-30:]):\n",
    "    print(idx2word[i], ' ', s.item(), end=' \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结论\n",
    "- attention 最大的30个单词都是一个句子中 与电影评价相关的词语 \n",
    "\n",
    "- attention 最小的30个单词 都是和对电影情感评价无关的词语"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析相同单词在不同语境下attention的变化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取词向量，和计算单词的权重参数\n",
    "word_embedding = model.get_embed_weigth().cpu()\n",
    "u = model.get_u().cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本总单词量为： 14828\n",
      "词频大于100的单词有：668\n"
     ]
    }
   ],
   "source": [
    "## 选取词频大于100的单词\n",
    "c = Counter()\n",
    "for sent in train_sentences:\n",
    "    for word in sent:\n",
    "        c[word] += 1\n",
    "print('文本总单词量为：',len(c))\n",
    "words = []\n",
    "for w in c:\n",
    "    if c[w] >=100:\n",
    "        words.append(w)\n",
    "print('词频大于100的单词有：{}'.format(len(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"new / from / the / no / wit / , / only / gags / that / its / characters / and / something / rather / beautiful / about / human / nature / remains / to / same / on / worst / clichés / filmmakers / could / up / 's / far / too / such / director / of / hollywood / as / can / still / turn / out / a / small / personal / film / with / an / emotional / . / are / more / deeply / through / than / in / most / ` / ' / films / goes / for / those / who / they / do / n't / make / movies / like / part / where / nothing / how / bad / this / movie / was / some / dumb / story / greatest / cold / his / usual / intelligence / concept / is / above / all / young / woman / face / by / whose / it / original / ways / even / if / anything / see / black / your / comes / performances / unfunny / cast / which / half / worse / : / or / world / cinema / very / good / plot / but / action / will / find / little / interest / often / year / sit / another / `` / best / man / '' / funny / adults / have / i / think / what / -- / real / between / silly / point / classic / every / sense / sometimes / come / already / having / been / times / 'd / care / feature / version / gorgeous / them / proves / once / again / he / has / lost / off / performance / beauty / any / ever / us / many / beautifully / contrived / situations / him / be / touching / less / rich / stuff / shot / ugly / video / ... / at / ; / ending / ? / though / our / just / not / pretty / we / never / feel / these / one / together / experience / men / ideas / women / lack / seem / fresh / history / earnest / few / laughs / love / power / people / almost / horror / life / much / fun / without / true / actors / two / when / first / ca / enough / comedy / so / manages / terrific / keep / family / engrossing / entertainment / written / you / believe / anyone / comic / melodrama / around / intriguing / moments / does / become / yet / ! / directed / certainly / eyes / away / images / long / recent / cinematic / quite / show / put / own / mystery / going / go / down / other / work / direction / would / had / piece / storytelling / takes / talented / create / watch / kind / after / laugh / -lrb- / -rrb- / completely / time / get / back / lot / charming / quirky / their / old / try / dialogue / bring / surprisingly / role / great / charm / there / artist / years / effort / art / engaging / itself / way / big / play / scene / look / ride / set / watching / rock / lacking / suspense / trying / important / special / welcome / filmmaking / easily / view / death / seen / heart / interesting / into / makes / sure / works / should / sequel / despite / themselves / compelling / picture / minutes / remarkable / pictures / romance / genre / better / short / shows / acting / music / least / want / being / able / 're / over / day / narrative / style / running / epic / fans / character / satisfying / while / both / considerable / flat / awful / actually / effects / thing / whole / mess / warmth / gentle / humor / working / sensitive / script / girl / enjoy / beyond / dark / entirely / give / feels / things / 've / before / predictable / brilliant / entertaining / change / tone / here / seems / because / may / inside / watchable / honest / thin / title / delightful / different / worth / idea / shallow / tedious / also / stylish / exercise / easy / thoroughly / hard / fascinating / camera / take / well / line / genuine / insight / my / hours / john / crime / full / warm / under / energy / home / leave / old-fashioned / viewers / drama / either / her / debut / fine / kids / everything / familiar / made / becomes / high / sex / excellent / culture / scenes / seeing / intelligent / moving / 'm / appeal / looking / right / summer / end / endearing / looks / days / especially / ambitious / along / filmmaker / subject / matter / always / creative / run / low / screen / next / three / soap / opera / simply / tv / leaves / enjoyable / powerful / wonderful / cinematography / reason / level / making / passion / material / formula / wrong / since / violence / mr. / himself / thriller / complex / writing / now / slow / imagination / appealing / serious / turns / then / historical / solid / modern / road / obvious / stupid / elements / mood / finally / children / emotionally / 'll / were / fairly / study / gets / job / done / book / deep / unsettling / found / audience / lead / star / spirit / master / war / screenplay / thoughtful / tale / lacks / theater / strong / nearly / probably / reality / sentimental / talent / stories / decent / might / dramatic / perfect / waste / romantic / comedies / flick / sweet / highly / knows / need / unique / attention / dull / really / portrait / rare / fails / mind / success / lives / winning / pleasure / feeling / why / each / none / convincing / viewer / period / adventure / everyone / documentary / satire / must / creepy / instead / smart / live / - / hour / truly / painful / american / colorful / fantasy / sequences / sort / audiences / bit / premise / clever / getting / hero / sad / me / ends / wo / journey / know / actor / gives / say / parents / particularly / tired / last / attempt / quality / otherwise / hackneyed / impressive / middle / act / animation / impossible / side / neither / provocative / visual / moral / social / place / boring / past / tragedy / cool / vision / head / psychological / bland / jokes / cheap / against / pretentious / perfectly / michael / ultimately / inventive / falls / nice / french / surprising / worthy / else / amusing / hilarious / political / message / light / simple / cliches / barely / plays / offers / nor / got / memorable / project / series / moment / left / visually / she / fact / did / imagine / believable / likely / de / seat\""
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' / '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_attention(sentence, word_embedding, u, word2idx):\n",
    "    '计算一个句子中每个单词在句子中的attention， 返回单词与attention值的字典'\n",
    "    num_sentence = [word2dix[w] for w in sentence]\n",
    "    s_embed = word_embedding[num_sentence]\n",
    "    u = u.repeat(s_embed.size(0),1)\n",
    "    score = torch.cosine_similarity(s_embed, u, dim=1)\n",
    "    attn = torch.softmax(score, dim=0)\n",
    "    return {w:a for w, a in zip(sentence, attn.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06510528922080994, 0.039851825684309006, 0.30102190375328064, 0.09700364619493484, 0.027896232903003693, 0.1262332648038864, 0.0618707612156868, 0.025379355996847153, 0.02642618678510189, 0.03359510004520416, 0.015629686415195465, 0.02155967801809311, 0.08656755089759827, 0.06489939987659454, 0.025537991896271706, 0.03268999978899956, 0.09740019589662552, 0.04514603689312935, 0.016434505581855774, 0.019962940365076065, 0.03635508194565773, 0.040564943104982376, 0.12715505063533783, 0.06926793605089188, 0.04552440717816353, 0.1024002805352211, 0.04836370050907135, 0.024082524701952934, 0.15549644827842712, 0.057421088218688965, 0.11364872753620148, 0.10440151393413544, 0.07316967844963074, 0.020314401015639305, 0.01879163458943367, 0.06838633865118027, 0.03276442363858223, 0.057646963745355606, 0.04209739714860916, 0.020052844658493996, 0.22116057574748993, 0.16049490869045258, 0.061901964247226715, 0.10461759567260742, 0.07503538578748703, 0.026572594419121742, 0.016548946499824524, 0.03251960501074791, 0.027706531807780266, 0.029943741858005524, 0.15133969485759735, 0.06318800151348114, 0.03407607972621918, 0.18141090869903564, 0.04192071408033371, 0.07955411076545715, 0.10000017285346985, 0.053883470594882965, 0.19896672666072845, 0.08236576616764069, 0.05500852316617966, 0.036844104528427124, 0.027025412768125534, 0.03602655604481697, 0.07586356997489929, 0.04866396263241768, 0.027749165892601013, 0.021459486335515976, 0.10810471326112747, 0.031610965728759766, 0.02862510271370411, 0.01795148476958275, 0.07760237902402878, 0.027837803587317467, 0.14069411158561707, 0.031374551355838776, 0.029985614120960236, 0.03920701518654823, 0.032775457948446274, 0.06067676842212677, 0.0396498441696167, 0.018794549629092216, 0.03892829641699791, 0.05544419214129448, 0.1320178359746933, 0.07466863840818405, 0.0768393725156784, 0.08256466686725616, 0.05716460570693016, 0.05568954721093178, 0.06011497229337692, 0.027691226452589035, 0.19676238298416138, 0.024766182526946068, 0.11470077931880951, 0.05694255232810974, 0.11501719802618027, 0.04731416702270508, 0.03277081996202469, 0.17513613402843475, 0.13821826875209808, 0.06864045560359955, 0.3314065933227539, 0.11550217866897583, 0.018048418685793877, 0.018572868779301643, 0.22183960676193237, 0.19252917170524597, 0.02994302287697792, 0.04588579013943672, 0.01883997954428196, 0.026354361325502396, 0.027095308527350426, 0.06116903945803642, 0.028015578165650368, 0.029034258797764778, 0.0768120139837265, 0.02938081882894039, 0.01948080211877823, 0.022711286321282387, 0.051008936017751694, 0.06607677042484283, 0.023447545245289803, 0.1049530878663063, 0.061915114521980286, 0.01453283429145813, 0.03663019463419914, 0.057332687079906464, 0.02188575454056263, 0.024256601929664612, 0.044199999421834946, 0.10136326402425766, 0.032783202826976776, 0.047545257955789566, 0.05862394720315933, 0.21702785789966583, 0.021297961473464966, 0.022424109280109406, 0.02075047418475151, 0.06783806532621384, 0.04297042265534401, 0.024667376652359962, 0.021969441324472427, 0.03075207583606243, 0.03974197059869766, 0.16063416004180908, 0.1682596057653427, 0.027255211025476456, 0.05068574845790863, 0.16566206514835358, 0.028048891574144363, 0.23852065205574036, 0.023107541725039482, 0.05304594710469246, 0.05408328399062157, 0.056042637676000595, 0.024809742346405983, 0.013550137169659138, 0.06967789679765701, 0.06350445002317429, 0.08040975779294968, 0.04686375707387924, 0.045768339186906815, 0.15667007863521576, 0.0446145199239254, 0.04481377825140953, 0.054364752024412155, 0.024052217602729797, 0.021345768123865128, 0.06340322643518448, 0.033746760338544846, 0.019042979925870895, 0.028443505987524986, 0.02612363174557686, 0.1515420526266098, 0.38837844133377075, 0.049732550978660583, 0.20810222625732422, 0.14462697505950928, 0.4118620455265045, 0.03783348947763443, 0.023276910185813904, 0.08057449758052826, 0.032255563884973526, 0.0316893570125103, 0.0869816467165947, 0.041157178580760956, 0.060469094663858414, 0.2798074185848236, 0.033660776913166046, 0.023944204673171043, 0.034852173179388046, 0.11048295348882675, 0.025291649624705315, 0.029176129028201103, 0.045379891991615295, 0.05902588739991188, 0.1379314810037613, 0.05150565877556801, 0.08854897320270538, 0.031795840710401535, 0.15460516512393951, 0.05384869500994682, 0.05734547600150108, 0.014686969108879566, 0.0917292907834053, 0.09577304869890213, 0.12765346467494965, 0.030948201194405556, 0.018827805295586586, 0.022130127996206284, 0.020969679579138756, 0.036382608115673065, 0.02819397859275341, 0.05410907417535782, 0.053594645112752914, 0.0711417868733406, 0.04303748905658722, 0.03550136834383011, 0.05057942867279053, 0.029165053740143776, 0.04418404400348663, 0.15743857622146606, 0.060207173228263855, 1.0, 0.022879330441355705, 0.22340653836727142, 0.04287354275584221, 0.04153238236904144, 0.07501170039176941, 0.09208622574806213, 0.019653378054499626, 0.04107965528964996, 0.024614613503217697, 0.14165841042995453, 0.17078587412834167, 0.04665546119213104, 0.015392003580927849, 0.029486393555998802, 0.03080933168530464, 0.039236024022102356, 0.021470079198479652, 0.027973556891083717, 0.09394284337759018, 0.15595167875289917, 0.05656475946307182, 0.0496741347014904, 0.031366363167762756, 0.021062711253762245, 0.02613472379744053, 0.05065710097551346, 0.03452519327402115, 0.12313305586576462, 0.029403790831565857, 0.14029687643051147, 0.09880658984184265, 0.033530838787555695, 0.04211211949586868, 0.07432220131158829, 0.02866392955183983, 0.039240673184394836, 0.0354890413582325, 0.018271204084157944, 0.030016470700502396, 0.0485798716545105, 0.037017010152339935, 0.022947439923882484, 0.024094004184007645, 0.058978114277124405, 0.017766928300261497, 0.06714005023241043, 0.15165041387081146, 0.018486682325601578, 0.08192973583936691, 0.049397505819797516, 0.02659032866358757, 0.07643267512321472, 0.042361464351415634, 0.042173340916633606, 0.023458825424313545, 0.08193277567625046, 0.09291175752878189, 0.04151081293821335, 0.03486081212759018, 0.019216716289520264, 0.07310467213392258, 0.03295162320137024, 0.04769570380449295, 0.08816912025213242, 0.03147764503955841, 0.021703731268644333, 0.016212020069360733, 0.09858161956071854, 0.03678101301193237, 0.01343041006475687, 0.07199525833129883, 0.03521276265382767, 0.08332236111164093, 0.11025996506214142, 0.027654439210891724, 0.02934262715280056, 0.05095578357577324, 0.02542794495820999, 0.05278308689594269, 0.021512571722269058, 0.05487918481230736, 0.019109832122921944, 0.034015968441963196, 0.024717865511775017, 0.07425489276647568, 0.04621817544102669, 0.13257275521755219, 0.025865919888019562, 0.027867162600159645, 0.04373713582754135, 0.032646745443344116, 0.03038410283625126, 0.02741672843694687, 0.032315101474523544, 0.04060543701052666, 0.03159363195300102, 0.03793388232588768, 0.01743241399526596, 0.05729931965470314, 0.030118415132164955, 0.0344511978328228, 0.01756404899060726, 0.03781949356198311, 0.017297351732850075, 0.2741830348968506, 0.12822209298610687, 0.22242020070552826, 0.03344929963350296, 0.03154261037707329, 0.021950628608465195, 0.17161259055137634, 0.035648323595523834, 0.12348724156618118, 0.10142359137535095, 0.024779152125120163, 0.17760896682739258, 0.17023319005966187, 0.049626801162958145, 0.04577084630727768, 0.026978500187397003, 0.020414991304278374, 0.04992794245481491, 0.04398111626505852, 0.041886698454618454, 0.03808361664414406, 0.05996936187148094, 0.022314460948109627, 0.14930786192417145, 0.017248162999749184, 0.0454685240983963, 0.02650357037782669, 0.014620179310441017, 0.033737484365701675, 0.3729844391345978, 0.04596269503235817, 0.06455721706151962, 0.06664048135280609, 0.06966688483953476, 0.034547727555036545, 0.050378937274217606, 0.015626413747668266, 0.02737320400774479, 0.032919615507125854, 0.036488305777311325, 0.06943845003843307, 0.09702300280332565, 0.017439059913158417, 0.03368636220693588, 0.10915365070104599, 0.025982007384300232, 0.06470198929309845, 0.017707452178001404, 0.047952570021152496, 0.021806754171848297, 0.08432631194591522, 0.030431807041168213, 0.03082522377371788, 0.15155185759067535, 0.0435035414993763, 0.04513143375515938, 0.044550225138664246, 0.060507144778966904, 0.05071602389216423, 0.3737739622592926, 0.340057373046875, 0.1811317503452301, 0.11995553970336914, 0.2523488402366638, 0.03972671553492546, 0.19623467326164246, 0.01894942857325077, 0.05003935471177101, 0.05000825226306915, 0.02647022157907486, 0.10148351639509201, 0.03237954154610634, 0.05753502622246742, 0.09940113127231598, 0.06882011890411377, 0.11326500028371811, 0.05969744548201561, 0.01967001147568226, 0.030916310846805573, 0.043160341680049896, 0.016446266323328018, 0.02607709728181362, 0.04894492030143738, 0.16648437082767487, 0.028315016999840736, 0.035852015018463135, 0.017436420544981956, 0.01840447634458542, 0.042106300592422485, 0.05940234661102295, 0.06402533501386642, 0.01950584165751934, 0.026459243148565292, 0.0938219502568245, 0.025306077674031258, 0.024631910026073456, 0.02663455903530121, 0.01816277578473091, 0.015206309966742992, 0.04562622681260109, 0.024239210411906242, 0.12987001240253448, 0.0191016998142004, 0.03850884735584259, 0.05114509165287018, 0.03225446864962578, 0.04926115274429321, 0.050531622022390366, 0.015058020129799843, 0.03314950689673424, 0.044002849608659744, 0.05664656311273575, 0.03349389508366585, 0.09067202359437943, 0.02840208075940609, 0.08025763928890228, 0.02654879167675972, 0.022220376878976822, 0.047149669378995895, 0.014433505944907665, 0.1472788006067276, 0.05333339422941208, 0.04702229052782059, 0.03472527489066124, 0.02547459863126278, 0.04639403894543648, 0.04975495859980583, 0.028586948290467262, 0.0573824979364872, 0.057154491543769836, 0.029750963672995567, 0.0603892020881176, 0.021920453757047653, 0.05782227963209152, 0.037931397557258606, 0.04046109318733215, 0.018570121377706528, 0.12737850844860077, 0.1869063824415207, 0.029287036508321762, 0.021405881270766258, 0.17430226504802704, 0.023769419640302658, 0.07915931195020676, 0.06991814076900482, 0.022289706394076347, 0.02064531482756138, 0.12245085090398788, 0.055354706943035126, 0.12314558774232864, 0.02855296991765499, 0.09129054844379425, 0.13022905588150024, 0.1290225088596344, 0.03923606872558594, 0.03259722515940666, 0.024545952677726746, 0.09047216922044754, 0.03747731074690819, 0.032175999134778976, 0.038007140159606934, 0.06687616556882858, 0.05295602232217789, 0.028531143441796303, 0.02197415567934513, 0.043629128485918045, 0.040399909019470215, 0.047235939651727676, 0.042191728949546814, 0.029178816825151443, 0.02275233343243599, 0.29148852825164795, 0.018391957506537437, 0.036685340106487274, 0.030879702419042587, 0.18881486356258392, 0.040746111422777176, 0.03428665176033974, 0.039465054869651794, 0.040317099541425705, 0.053294576704502106, 0.029945850372314453, 0.019976027309894562, 0.2434399574995041, 0.01702232100069523, 0.19506773352622986, 0.030106086283922195, 0.04356280341744423, 0.015304595232009888, 0.029171885922551155, 0.12603802978992462, 0.03512682765722275, 0.05854817107319832, 0.04311651363968849, 0.08173556625843048, 0.017513331025838852, 0.0293227918446064, 0.03497650474309921, 0.05097602680325508, 0.019177470356225967, 0.028091473504900932, 0.06049157306551933, 0.04381449520587921, 0.11605948209762573, 0.05287715420126915, 0.027115358039736748, 0.028976835310459137, 0.018657037988305092, 0.04977992922067642, 0.23666132986545563, 0.014041182585060596, 0.06564457714557648, 0.05046993866562843, 0.026998598128557205, 0.021302390843629837, 0.02077355608344078, 0.029491985216736794, 0.06806712597608566, 0.04005136713385582, 0.0464630052447319, 0.013927653431892395, 0.05255058407783508, 0.05015873536467552, 0.03203504532575607, 0.10525038838386536, 0.031698618084192276, 0.03430773317813873, 0.15157140791416168, 0.02643345296382904, 0.055754709988832474, 0.02216518484055996, 0.0593661330640316, 0.08628969639539719, 0.08163303881883621, 0.038351595401763916, 0.041340671479701996, 0.061922669410705566, 0.20967204868793488, 0.021138502284884453, 0.04597543925046921, 0.03915970399975777, 0.021622857078909874, 0.03268743306398392, 0.029361644759774208, 0.16072559356689453, 0.025721171870827675, 0.0450664758682251, 0.22066152095794678, 0.07527008652687073, 0.14914025366306305, 0.032520364969968796, 0.1485820710659027, 0.027134882286190987, 0.033284854143857956, 0.03063984401524067, 0.3243200182914734, 0.02709992788732052, 0.24113287031650543, 0.14023537933826447, 0.07437743246555328, 0.08463972061872482, 0.030833987519145012, 0.03506295382976532, 0.021817229688167572, 0.021464340388774872, 0.01669306494295597, 0.02426191046833992, 0.03330232948064804, 0.04474087059497833, 0.11673643440008163, 0.02298189513385296, 0.04700903221964836, 0.03368997946381569, 0.06282542645931244, 0.05672507733106613, 0.0309324748814106, 0.02266242355108261, 0.41805675625801086, 0.070411317050457, 0.034319546073675156, 0.09793733060359955, 0.050086162984371185, 0.05342939868569374, 0.01915326528251171, 0.051065877079963684, 0.04572167620062828, 0.053069356828927994, 0.08557119965553284, 0.10013311356306076, 0.06225138157606125, 1.0, 0.017789771780371666, 0.018241168931126595, 0.0431341715157032, 0.04766745865345001, 0.08822358399629593, 0.12923504412174225, 0.23496614396572113, 0.13065366446971893, 0.19384875893592834, 0.01481025479733944, 0.03301790729165077, 0.11447544395923615, 0.026026051491498947, 0.022889679297804832, 0.07793788611888885, 0.02594415657222271, 0.09346648305654526, 0.049087852239608765, 0.1730649471282959, 0.5278456807136536, 0.032524473965168, 0.04842916876077652, 0.10804944485425949, 0.2073432207107544, 0.04390925168991089, 0.06417375057935715, 0.015513010323047638, 0.06376910954713821, 0.05669976770877838, 0.1777622103691101, 0.03758292272686958, 0.21774932742118835, 0.017805522307753563, 0.06108032539486885, 0.02714207023382187, 0.1932813823223114, 0.017286157235503197, 0.024751970544457436, 0.486902117729187, 0.030802929773926735, 0.03053360804915428, 0.052809614688158035, 0.02720865048468113, 0.18277066946029663, 0.05707951635122299, 0.020823445171117783, 0.0938853994011879, 0.042807161808013916, 0.018897926434874535, 0.013273916207253933, 0.04503437131643295, 0.08390801399946213, 0.04538610577583313, 0.034583546221256256]\n"
     ]
    }
   ],
   "source": [
    "## 初始化单词attention 列表为空，将所有单词做成一个字典\n",
    "word_attention_li = {w:[] for w in words}\n",
    "print(word_attention_li['new'])\n",
    "for s in train_sentences:\n",
    "    s_w_attn = get_attention(s, word_embedding, u, word2dix)\n",
    "    for w in s_w_attn:\n",
    "        if w in word_attention_li:\n",
    "            word_attention_li[w].append(s_w_attn[w])\n",
    "print(word_attention_li['new'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674\n"
     ]
    }
   ],
   "source": [
    "print(len(word_attention_li['new']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_std_list(word_attention_li, sort=True):\n",
    "    '计算attention的均值和标准差，并按照标准差排序'\n",
    "    import numpy as np\n",
    "    word_mean_std_li=[]\n",
    "    for w in word_attention_li:\n",
    "        arr = np.array(word_attention_li[w])\n",
    "        word_mean_std_li.append((w, arr.mean(), arr.std()))\n",
    "    if sort:\n",
    "        sorted_std_li = sorted(word_mean_std_li, key=lambda x:x[2], reverse=True)\n",
    "        sorted_mean_li = sorted(word_mean_std_li, key=lambda x:x[1], reverse=True)\n",
    "    else:\n",
    "        return word_mean_std_li\n",
    "    return sorted_std_li, sorted_mean_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_std_li, sorted_mean_li= mean_std_list(word_attention_li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30个标准差最大的单词：\n",
      "stupid : 0.2377\n",
      "awful : 0.234\n",
      "terrific : 0.2326\n",
      "tedious : 0.2304\n",
      "watchable : 0.2272\n",
      "provocative : 0.222\n",
      "flat : 0.2218\n",
      "painful : 0.221\n",
      "inventive : 0.2205\n",
      "bland : 0.2201\n",
      "boring : 0.2197\n",
      "appealing : 0.2171\n",
      "waste : 0.2168\n",
      "gorgeous : 0.2158\n",
      "remarkable : 0.2154\n",
      "excellent : 0.2145\n",
      "mess : 0.2145\n",
      "worse : 0.2141\n",
      "beautifully : 0.2127\n",
      "unfunny : 0.2111\n",
      "impressive : 0.209\n",
      "brilliant : 0.2076\n",
      "intriguing : 0.2066\n",
      "convincing : 0.2036\n",
      "slow : 0.2034\n",
      "cool : 0.2032\n",
      "engrossing : 0.2026\n",
      "wonderful : 0.2015\n",
      "delightful : 0.2012\n",
      "bad : 0.2009\n"
     ]
    }
   ],
   "source": [
    "print('30个标准差最大的单词：')\n",
    "for word, amean, astd in sorted_std_li[:30]:\n",
    "    print('{} : {:.4}'.format(word, astd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30个标准差最小的单词：\n",
      "of : 0.03817\n",
      "if : 0.03788\n",
      "adults : 0.03738\n",
      "whose : 0.03656\n",
      "about : 0.03644\n",
      "i : 0.03643\n",
      "but : 0.03612\n",
      "-- : 0.03554\n",
      "though : 0.0352\n",
      ". : 0.03507\n",
      "at : 0.03471\n",
      "while : 0.03463\n",
      "filmmakers : 0.03399\n",
      "had : 0.03391\n",
      "to : 0.03372\n",
      "ever : 0.03367\n",
      "shows : 0.03364\n",
      "de : 0.0335\n",
      "they : 0.03286\n",
      "we : 0.03205\n",
      "now : 0.03194\n",
      "mr. : 0.03096\n",
      "into : 0.03038\n",
      ": : 0.03024\n",
      "that : 0.02741\n",
      "because : 0.02642\n",
      "which : 0.02581\n",
      "who : 0.02491\n",
      "when : 0.02489\n",
      "; : 0.0237\n"
     ]
    }
   ],
   "source": [
    "print('30个标准差最小的单词：')\n",
    "for word, amean, astd in word_mean_std_li[-30:]:\n",
    "    print('{} : {:.4}'.format(word, astd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30个均值最大的单词：\n",
      "mess : 0.3365\n",
      "wrong : 0.324\n",
      "stupid : 0.3218\n",
      "awful : 0.3215\n",
      "waste : 0.3172\n",
      "terrific : 0.3164\n",
      "brilliant : 0.3059\n",
      "tired : 0.3039\n",
      "unfunny : 0.3021\n",
      "bad : 0.3021\n",
      "touching : 0.2984\n",
      "worst : 0.2979\n",
      "provocative : 0.2962\n",
      "engrossing : 0.2907\n",
      "slow : 0.2906\n",
      "excellent : 0.2885\n",
      "impressive : 0.288\n",
      "remarkable : 0.2864\n",
      "boring : 0.2827\n",
      "beautifully : 0.2813\n",
      "watchable : 0.281\n",
      "flat : 0.2797\n",
      "wonderful : 0.2792\n",
      "hilarious : 0.2788\n",
      "fascinating : 0.2783\n",
      "painful : 0.2768\n",
      "inventive : 0.2749\n",
      "appealing : 0.2741\n",
      "pretentious : 0.2741\n",
      "delightful : 0.2739\n"
     ]
    }
   ],
   "source": [
    "print('30个均值最大的单词：')\n",
    "for word, amean, astd in sorted_mean_li[:30]:\n",
    "    print('{} : {:.4}'.format(word, amean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30个均值最小的单词：\n",
      "shows : 0.04875\n",
      "... : 0.04869\n",
      "times : 0.04832\n",
      "had : 0.04819\n",
      "filmmakers : 0.04786\n",
      "ever : 0.04786\n",
      "-rrb- : 0.04785\n",
      "i : 0.04767\n",
      "though : 0.04661\n",
      "de : 0.046\n",
      "from : 0.04579\n",
      "which : 0.0457\n",
      "whose : 0.04494\n",
      ". : 0.04483\n",
      "about : 0.04473\n",
      "they : 0.04469\n",
      "to : 0.04457\n",
      "'' : 0.04442\n",
      "but : 0.04431\n",
      "at : 0.04378\n",
      "when : 0.04286\n",
      "we : 0.04208\n",
      "into : 0.04168\n",
      "-- : 0.04111\n",
      "-lrb- : 0.04107\n",
      ": : 0.04001\n",
      "because : 0.03962\n",
      "that : 0.03814\n",
      "who : 0.03568\n",
      "; : 0.03429\n"
     ]
    }
   ],
   "source": [
    "print('30个均值最小的单词：')\n",
    "for word, amean, astd in sorted_mean_li[-30:]:\n",
    "    print('{} : {:.4}'.format(word, amean))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析\n",
    "均值和标准差代表一组数据的分布情况， 从上面可以看出对于一些不重要的词，词向量的attention的均值和标准差都比较小，说明在每一句话中这些单词的attention 都不高。对于词向量和attention 都比较大的词 都是和评价有关的词语，这些词在句子中占有较大得attention，当然标准差也比较大说明在不同句子中的attention 值不一样，这是根据评论语句内容有关，如果评价语句中出现了不止一个重要的单词，那么attention也会分给其他单词，所以在某一个单词上的attention会减小，如果评价语句中只有一个重要的单词，那么这个单词的attention会非常高。\n",
    "\n",
    "例如： 我们以‘stupid’为例，把所有含有stupid的句子取出来，看每个句子中 stupid的attention 大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'simply': [0.14071136713027954, 0.1102457121014595, 0.0955563634634018, 0.11794734746217728, 0.06250448524951935, 0.060315825045108795, 0.12973597645759583, 0.09049715846776962, 0.05814916640520096, 0.23121154308319092, 0.10356587171554565], 'stupid': [0.38554927706718445, 0.12267553061246872, 0.3020733594894409, 0.6308980584144592, 0.06658100336790085, 0.13114982843399048, 0.09470311552286148, 0.10759475827217102, 0.5478944182395935, 0.8061841130256653, 0.06278679519891739, 0.20323993265628815, 0.26182451844215393, 0.09664229303598404, 0.587374746799469, 0.323175847530365, 0.17126235365867615, 0.4033791422843933, 0.3887804448604584, 0.40185871720314026, 0.16526541113853455, 0.07669458538293839, 0.3554767072200775, 0.5137991905212402, 0.10788409411907196, 0.78346186876297, 0.7512186169624329, 0.09395401179790497, 0.800701379776001, 0.2036629319190979, 0.8237711787223816, 0.49585697054862976, 0.4442058503627777, 0.23981519043445587, 0.10816267877817154, 0.23349826037883759, 0.2577085793018341, 0.07804407179355621, 0.7088738679885864, 0.44486135244369507, 0.305471271276474, 0.5786352157592773, 0.6131293773651123, 0.2869524657726288, 0.6581787467002869, 1.0, 0.40547922253608704, 0.7657994031906128, 0.06211848556995392, 0.09753934293985367, 0.13773369789123535, 0.44391199946403503, 0.22821901738643646, 0.08074133098125458, 0.3703272044658661, 0.2683190107345581, 0.08705271035432816, 0.15245948731899261, 0.16876907646656036, 0.3998126983642578, 0.23857834935188293, 0.0744263157248497, 0.1106908842921257, 0.5363026261329651, 0.43824437260627747, 0.2675773799419403, 0.07058679312467575, 0.06565481424331665, 0.1672833114862442, 0.22414197027683258, 0.06713598966598511, 0.1320648491382599, 0.1297646164894104, 0.14770428836345673, 0.25128045678138733, 0.5077778100967407, 0.4407324492931366, 0.8240495920181274, 0.25393128395080566, 0.11034628748893738, 0.18181368708610535, 0.6936635375022888, 0.15484601259231567, 0.08546370267868042, 0.1033768281340599, 0.09569261223077774, 0.4427518844604492, 0.18550170958042145, 0.14486443996429443, 0.1337944120168686, 0.24796229600906372, 0.404511034488678, 0.2798386514186859, 0.6056569814682007, 0.7068068981170654, 0.20460370182991028, 0.16914644837379456, 0.8222578763961792, 0.1501546949148178, 0.23173819482326508, 0.15932875871658325, 0.8240495920181274, 0.4834858477115631, 0.22081227600574493, 0.7050910592079163, 0.1783759444952011, 0.2223779261112213, 1.0, 0.49035701155662537, 0.3262757360935211, 0.10367858409881592, 0.2058711051940918, 0.11352398991584778, 0.17968744039535522, 0.6335198283195496, 0.18570968508720398, 0.3016372621059418, 0.1520555019378662, 0.18843136727809906, 0.5426362156867981, 0.3287999629974365, 0.2561054229736328, 0.07504859566688538, 0.3210461437702179, 0.4849787652492523, 0.060545653104782104, 0.07935629785060883, 0.213875412940979, 0.0762612372636795, 0.14970165491104126, 0.1334332823753357, 0.540003776550293, 0.7876837849617004, 0.8200607299804688, 0.2157662957906723, 0.2837705910205841, 0.045822471380233765], ',': [0.08232218027114868, 0.06449846923351288, 0.020220935344696045, 0.1169859766960144, 0.013406187295913696, 0.04339562729001045, 0.0559045672416687, 0.020634986460208893, 0.06900425255298615, 0.0365678034722805, 0.08612918853759766, 0.035287342965602875, 0.0759011059999466, 0.020060986280441284, 0.04348594695329666, 0.09484647959470749, 0.05120514705777168, 0.049856361001729965, 0.061269860714673996, 0.14053380489349365, 0.08657759428024292, 0.0947837382555008, 0.04872914031147957, 0.03255302831530571, 0.036035437136888504, 0.05094105750322342, 0.1145109087228775, 0.09357358515262604, 0.05713291093707085, 0.015071636065840721, 0.04785861447453499, 0.01433482300490141, 0.027707237750291824, 0.031537700444459915, 0.05365320295095444, 0.0941048413515091, 0.17595040798187256, 0.02356105111539364, 0.03882071375846863, 0.052944716066122055, 0.08637087792158127, 0.05975092574954033, 0.043686818331480026, 0.036116015166044235, 0.032060910016298294, 0.04948055371642113, 0.03401975333690643, 0.17595040798187256, 0.047147661447525024, 0.03808669000864029, 0.0474819578230381, 0.10470063984394073, 0.06966613233089447, 0.03836672008037567, 0.13526864349842072, 0.03965258225798607, 0.06440534442663193, 0.032466769218444824, 0.04023371636867523, 0.07020510733127594, 0.05468342453241348, 0.01602431759238243, 0.0685495138168335, 0.012927659787237644, 0.04566650465130806, 0.016283240169286728, 0.03196417912840843, 0.028490567579865456, 0.0605904720723629, 0.009783978573977947], 'irrelevant': [0.39141714572906494, 0.30667075514793396, 0.2658093571662903, 0.3280944228172302, 0.17386887967586517, 0.16778066754341125, 0.3608868718147278, 0.2517361640930176, 0.16175366938114166, 0.28808942437171936], 'movies': [0.033914968371391296, 0.036257777363061905, 0.04280884563922882, 0.02857963554561138, 0.05128394067287445], 'with': [0.030700407922267914, 0.0166623592376709, 0.02692633680999279, 0.023512644693255424, 0.01554558239877224, 0.02440989762544632, 0.01862570270895958, 0.016430573537945747, 0.02587077207863331, 0.023947741836309433, 0.02594628557562828], 'the': [0.025476450100541115, 0.02723633497953415, 0.013039149343967438, 0.04229538142681122, 0.013942360877990723, 0.03215740621089935, 0.021468622609972954, 0.038523778319358826, 0.035127222537994385, 0.03731631115078926, 0.03913218900561333, 0.015585599467158318, 0.012573722749948502, 0.01583743281662464, 0.009516109712421894], 'courage': [0.09146507829427719, 0.09778338670730591, 0.0770762488245964], 'to': [0.026201026514172554, 0.014220362529158592, 0.028010966256260872, 0.013409996405243874, 0.04340796172618866, 0.016380421817302704, 0.023041870445013046, 0.04349830746650696, 0.02310137078166008, 0.016668643802404404, 0.013267259113490582, 0.020832441747188568, 0.029417144134640694, 0.017244724556803703, 0.018592707812786102, 0.015895964577794075, 0.023641346022486687, 0.014022547751665115, 0.014338896609842777, 0.018253328278660774, 0.022079212591052055, 0.020438015460968018, 0.028575794771313667, 0.0361262746155262, 0.03207002207636833, 0.04396992549300194, 0.024246439337730408, 0.012931332923471928, 0.016948910430073738, 0.04608334228396416, 0.009786758571863174], 'go': [0.08241961151361465, 0.08811306953430176, 0.10403336584568024, 0.06945376843214035], 'over': [0.04850493744015694, 0.05185560882091522, 0.06122489646077156, 0.040874384343624115], 'top': [0.0919032096862793, 0.09825178235769272, 0.11600394546985626, 0.07744545489549637, 0.13896986842155457], 'and': [0.02691769227385521, 0.06628149747848511, 0.014609327539801598, 0.02877713553607464, 0.020779933780431747, 0.02360864356160164, 0.05745002254843712, 0.02120543085038662, 0.07091184705495834, 0.037578705698251724, 0.08817660063505173, 0.03626284375786781, 0.07799936085939407, 0.023672128096222878, 0.020615562796592712, 0.05262069031596184, 0.09761231392621994, 0.013630153611302376, 0.02140226401388645, 0.030221782624721527, 0.050076235085725784, 0.0812578871846199, 0.0370316244661808, 0.016330761834979057, 0.015488284640014172, 0.014406101778149605, 0.036705613136291504, 0.04918164759874344, 0.02897791378200054, 0.03397659957408905, 0.02268313430249691, 0.020997051149606705, 0.09714943915605545, 0.040703125298023224, 0.03178642317652702, 0.029357418417930603, 0.054408349096775055, 0.08875855803489685, 0.04489452391862869, 0.03496021404862404, 0.039139579981565475, 0.07159202545881271, 0.022749347612261772, 0.03942735120654106, 0.04074876382946968, 0.033364299684762955, 0.04134596139192581, 0.04692893847823143, 0.0328478179872036, 0.17993924021720886, 0.06226546689867973, 0.0100544523447752], 'that': [0.02590487152338028, 0.027694351971149445, 0.019998054951429367, 0.013258421793580055, 0.0204075425863266, 0.0161952693015337, 0.022840255871415138, 0.016480235382914543, 0.01704980432987213, 0.01838255114853382, 0.03219422325491905, 0.023374127224087715, 0.014905514195561409, 0.014176822267472744, 0.027401844039559364, 0.031190088018774986, 0.05362159386277199, 0.02330135740339756, 0.038392823189496994, 0.03269817680120468, 0.01804700680077076, 0.021829646080732346, 0.03917160630226135, 0.02397238090634346, 0.054080694913864136, 0.012785169295966625, 0.016757331788539886, 0.02817654050886631, 0.009676137939095497], 'do': [0.09692545235157013, 0.10362095385789871, 0.21199777722358704, 0.2006300985813141, 0.12234321981668472, 0.0816776230931282, 0.14656415581703186], \"n't\": [0.14258724451065063, 0.1524370163679123, 0.11007457226514816, 0.11232849955558777, 0.31187039613723755, 0.08204387873411179, 0.2951473891735077, 0.1799793839454651, 0.12015613168478012, 0.21561086177825928], 'care': [0.034882452338933945, 0.03729209303855896, 0.07629577070474625, 0.07220466434955597, 0.04403004050254822, 0.02939491905272007, 0.05274690315127373], 'about': [0.026223834604024887, 0.028035348281264305, 0.023061927407979965, 0.05735742673277855, 0.05428182706236839, 0.033100783824920654, 0.02209843136370182, 0.020455805584788322, 0.039653923362493515, 0.028600668534636497], 'being': [0.03390583023428917, 0.03624800965189934, 0.21653810143470764, 0.07415968924760818, 0.07018312066793442, 0.04279731214046478, 0.02857193537056446, 0.051270123571157455], \"'s\": [0.06529725342988968, 0.01357221882790327, 0.05659692361950874, 0.03702068328857422, 0.08404017239809036, 0.03572436049580574, 0.01657857559621334, 0.023380830883979797, 0.0168702844530344, 0.13253623247146606, 0.023927336558699608, 0.09473246335983276, 0.01451235543936491, 0.14994478225708008, 0.05360041558742523, 0.17774218320846558, 0.03444107621908188, 0.10451202094554901, 0.024539751932024956, 0.1172981783747673, 0.016222773119807243, 0.06939847767353058, 0.01308776531368494, 0.017153939232230186, 0.016484903171658516, 0.06134086102247238, 0.009905150160193443], 'deeply': [0.08493292331695557, 0.07361629605293274, 0.09086623787879944, 0.04815324395895004, 0.04646710678935051, 0.06971870362758636, 0.04479791969060898, 0.07978679239749908], 'be': [0.21742680668830872, 0.021638255566358566, 0.03718016669154167, 0.047467250376939774, 0.02313711866736412, 0.032978605479002, 0.046109654009342194, 0.0709494948387146, 0.02086588740348816, 0.07435968518257141, 0.01579182781279087], 'so': [0.15167511999607086, 0.19381588697433472, 0.025936568155884743, 0.05613565444946289, 0.03311275318264961, 0.036653004586696625, 0.12893329560756683, 0.016969872638583183, 0.031196899712085724, 0.035509802401065826, 0.026528511196374893, 0.023005597293376923, 0.032165706157684326, 0.1456068754196167, 0.0534621998667717, 0.04949376359581947, 0.07904727756977081, 0.03207888826727867, 0.05187268182635307], 'more': [0.017133764922618866, 0.01598539389669895, 0.016895420849323273], 'than': [0.019601114094257355, 0.01828736998140812, 0.019328447058796883], 'a': [0.01398913748562336, 0.022606415674090385, 0.0131919477134943, 0.08475280553102493, 0.016114072874188423, 0.022667206823825836, 0.019740398973226547, 0.022725742310285568, 0.016397610306739807, 0.01305153127759695, 0.020493702962994576, 0.016964323818683624, 0.018290389329195023, 0.015637492761015892, 0.0232569370418787, 0.014830783009529114, 0.01379453856498003, 0.03514740616083145, 0.014105742797255516, 0.027747757732868195, 0.023184534162282944, 0.03820033743977547, 0.017956526950001717, 0.021720200777053833, 0.020105689764022827, 0.030437041074037552, 0.058796077966690063, 0.021783599629998207, 0.023852191865444183, 0.06337612122297287, 0.05380955711007118, 0.1018974632024765, 0.012721067294478416, 0.016673319041728973, 0.009627623483538628], 'widget': [0.054686445742845535, 0.05102114751935005, 0.05392571538686752], 'cranked': [0.05793435126543045, 0.054051365703344345, 0.06476082652807236, 0.05712844431400299], 'out': [0.02109628915786743, 0.034091558307409286, 0.01968233287334442, 0.030905483290553093, 0.023582087829709053, 0.020802823826670647, 0.032850708812475204], 'on': [0.018459761515259743, 0.01740780845284462, 0.021263781934976578, 0.142452210187912, 0.07145039737224579, 0.02163792960345745, 0.08469273895025253, 0.16042818129062653, 0.017222518101334572, 0.022385751828551292, 0.02413559891283512, 0.02063489705324173, 0.018202973529696465, 0.018613632768392563, 0.023695042356848717, 0.016786444932222366, 0.02200174890458584, 0.012704403139650822], 'an': [0.016137974336743355, 0.049261510372161865, 0.02614906057715416, 0.015056345611810684, 0.033384036272764206, 0.09690701961517334, 0.057826872915029526, 0.018039530143141747, 0.01591348461806774, 0.06090562045574188, 0.023194076493382454, 0.03242923319339752, 0.03639465197920799], 'assembly': [0.06019569933414459, 0.056161150336265564, 0.06728862971067429, 0.05935833603143692], 'line': [0.03815882280468941, 0.03560126945376396, 0.04265512153506279, 0.03762800619006157], 'see': [0.026199553161859512, 0.030179237946867943, 0.030710259452462196, 0.024443557485938072, 0.03838162496685982, 0.03177162632346153, 0.034255146980285645, 0.02928667515516281, 0.025835096836090088, 0.033629875630140305, 0.031226618215441704], 'if': [0.023374035954475403, 0.021807417273521423, 0.03424232080578804, 0.026128225028514862, 0.02304888516664505, 0.03639757260680199], 'americans': [0.017946574836969376, 0.0290016271173954, 0.016743725165724754, 0.026291240006685257, 0.02006123960018158, 0.01769692450761795, 0.02794604003429413, 0.21231617033481598], 'will': [0.020203847438097, 0.03264937549829483, 0.018849706277251244, 0.02959808148443699, 0.02258448861539364, 0.01992279663681984, 0.031461019068956375], 'get': [0.027598854154348373, 0.04459968954324722, 0.04471961781382561, 0.025749072432518005, 0.040431562811136246, 0.05709273740649223, 0.030850857496261597, 0.027214933186769485, 0.0396660640835762, 0.055459845811128616, 0.04297637194395065, 0.08533674478530884, 0.08943846076726913], 'kick': [0.04781213775277138, 0.07726430892944336, 0.044607583433389664, 0.07004346698522568, 0.05344589427113533, 0.04714703559875488, 0.07445208728313446], 'of': [0.014740250073373318, 0.02096615359187126, 0.023820212110877037, 0.02139546535909176, 0.013752301223576069, 0.021594060584902763, 0.037363484501838684, 0.016477109864354134, 0.01562708429992199, 0.014535202644765377, 0.03949033096432686, 0.02295321598649025, 0.033663295209407806, 0.016614874824881554, 0.01688333973288536], 'goofy': [0.016310788691043854, 0.02635820023715496, 0.015217576175928116, 0.023894857615232468, 0.01823270693421364, 0.016083894297480583, 0.025398828089237213], 'brits': [0.022930165752768517, 0.03705509752035141, 0.021393295377492905, 0.033592063933610916, 0.02563205175101757, 0.022611189633607864, 0.03570638597011566], 'cute': [0.04031240940093994, 0.06514476984739304, 0.03761051595211029, 0.05905657634139061, 0.04506246745586395, 0.03975163772702217, 0.06277366727590561], 'accents': [0.032571110874414444, 0.05263484641909599, 0.03038806840777397, 0.04771578311920166, 0.03640900179743767, 0.032118022441864014, 0.050719067454338074], 'performing': [0.05724884569644928, 0.09251400828361511, 0.053411804139614105, 0.08386798202991486, 0.06399454176425934, 0.056452471762895584, 0.08914673328399658], 'ages-old': [0.053463149815797806, 0.08639633655548096, 0.04987984150648117, 0.07832204550504684, 0.05976277217268944, 0.052719440311193466, 0.08325172960758209], 'slapstick': [0.0312371663749218, 0.05047919601202011, 0.029143530875444412, 0.0457615926861763, 0.03491787612438202, 0.030802635475993156, 0.0486418791115284], 'unfunny': [0.06904149055480957, 0.11157090216875076, 0.06441406160593033, 0.10114388912916183, 0.07717672735452652, 0.0680810734629631, 0.10751000046730042], 'tricks': [0.03445776179432869, 0.055683668702840805, 0.032148268073797226, 0.050479672849178314, 0.03851795569062233, 0.033978428691625595, 0.0536569207906723], 'played': [0.06167915090918541, 0.06294211745262146, 0.045972440391778946, 0.09903216361999512], 'game': [0.030221566557884216, 0.030840395018458366, 0.05385741963982582, 0.022525588050484657, 0.04852380231022835], 'absurd': [0.08635105192661285, 0.08811920881271362, 0.15388499200344086, 0.06436160206794739, 0.16264460980892181, 0.16933156549930573, 0.13864541053771973], 'plot': [0.04136532172560692, 0.04221233353018761, 0.07371655106544495, 0.030831564217805862, 0.07791272550821304, 0.08111602813005447, 0.06641623377799988, 0.020014772191643715], 'twists': [0.06182287633419037, 0.0630887821316719, 0.11017366498708725, 0.04607956483960152, 0.11644509434700012, 0.12123261392116547, 0.09926292300224304], 'idiotic': [0.0942842960357666, 0.0962148904800415, 0.16802269220352173, 0.07027462124824524, 0.17758707702159882, 0.18488839268684387, 0.15138304233551025], 'court': [0.03576461225748062, 0.03649694100022316, 0.06373561173677444, 0.026657087728381157, 0.06736364215612411, 0.07013323158025742, 0.05742373317480087], 'maneuvers': [0.03440060839056969, 0.035105008631944656, 0.061304837465286255, 0.02564043179154396, 0.06479450315237045, 0.06745845824480057, 0.055233683437108994], 'characters': [0.02025976963341236, 0.02067461609840393, 0.020099513232707977, 0.17622879147529602, 0.03610464930534363, 0.015100582502782345, 0.03815983980894089, 0.039728742092847824, 0.03252912685275078], 'even': [0.025673607364296913, 0.026199309155344963, 0.019135775044560432], 'freeman': [0.024866951629519463, 0.025376135483384132, 0.01853453554213047], 'ca': [0.07246973365545273, 0.07395365089178085, 0.054015181958675385], 'save': [0.0826806053519249, 0.0843736082315445, 0.06162583827972412], 'it': [0.021351534873247147, 0.014155758544802666, 0.021788736805319786, 0.017291374504566193, 0.024386093020439148, 0.034373145550489426, 0.024956097826361656, 0.0988055095076561, 0.015914326533675194, 0.01513631734997034, 0.029256414622068405, 0.03330104798078537, 0.024878405034542084, 0.1563916951417923, 0.03592187911272049, 0.10900554060935974, 0.01692027412354946, 0.07238227874040604, 0.01365047600120306, 0.01719367317855358, 0.030083542689681053, 0.01033102348446846], '.': [0.020065516233444214, 0.03501611948013306, 0.016249902546405792, 0.019906794652342796, 0.04315171390771866, 0.09411748498678207, 0.01653582975268364, 0.013161545619368553, 0.048354607075452805, 0.07846421003341675, 0.050549525767564774, 0.09285438060760498, 0.01495579443871975, 0.013910816051065922, 0.0274942796677351, 0.023379961028695107, 0.03852233663201332, 0.020275166258215904, 0.04335103929042816, 0.03583842143416405, 0.031814489513635635, 0.03375827148556709, 0.10244005918502808, 0.03807183355093002, 0.03992447629570961, 0.11497272551059723, 0.015901152044534683, 0.06802264600992203, 0.03171849623322487], 'really': [0.16755978763103485, 0.06228520721197128, 0.1358492374420166, 0.20128749310970306, 0.234200581908226, 0.05172920599579811, 0.1499634087085724], 'assumes': [0.06478175520896912, 0.062469400465488434, 0.04727841168642044], 'not': [0.0825427919626236, 0.14183004200458527, 0.18107187747955322, 0.0882604643702507, 0.12580248713493347, 0.17589309811592102, 0.27064886689186096, 0.09866279363632202, 0.07959646731615067, 0.10025699436664581, 0.28365761041641235, 0.06024060770869255], 'only': [0.03687542676925659, 0.03942975774407387, 0.03555917739868164, 0.026912080124020576], 'would': [0.03415345400571823, 0.058684539049863815, 0.07492149621248245, 0.036519233137369156, 0.05205287039279938, 0.07277869433164597, 0.1119854673743248, 0.03293436020612717, 0.11736804991960526, 0.024925552308559418], 'subtlety': [0.013744224794209003, 0.014696276746690273, 0.013253632001578808, 0.010030681267380714], 'lost': [0.0565776601433754, 0.060496747493743896, 0.05455814674496651, 0.041290976107120514], 'target': [0.022495299577713013, 0.024053530767560005, 0.021692341193556786, 0.016417309641838074], 'audience': [0.014735436998307705, 0.01575614884495735, 0.014209462329745293, 0.01075407862663269], 'but': [0.014859232120215893, 0.0158885195851326, 0.01776113174855709, 0.014328839257359505, 0.0180481169372797, 0.010844425298273563], 'also': [0.02130177989602089, 0.13190218806266785, 0.036696530878543854, 0.03755428269505501, 0.022777337580919266, 0.038515474647283554, 0.020541423931717873, 0.015546265989542007], 'too': [0.0638360008597374, 0.3952771723270416, 0.5041429996490479, 0.10997013002634048, 0.1125405877828598, 0.06825786828994751, 0.11542103439569473, 0.061557404696941376, 0.04658818989992142], 'realize': [0.04639585316181183, 0.079926036298275, 0.08179423958063126, 0.049609653651714325, 0.0838877409696579, 0.044739775359630585, 0.033860184252262115], 'they': [0.015475165098905563, 0.026659030467271805, 0.02728216163814068, 0.01654711738228798, 0.027980441227555275, 0.014922786504030228, 0.011293940246105194], \"'ve\": [0.01939140260219574, 0.033405523747205734, 0.03418635204434395, 0.020734628662467003, 0.035061340779066086, 0.01869923621416092, 0.014152051880955696], 'already': [0.0428476557135582, 0.0738135576248169, 0.07553888857364655, 0.04581567645072937, 0.07747228443622589, 0.04131822660565376, 0.031270675361156464], 'seen': [0.014723911881446838, 0.025364847853779793, 0.025957729667425156, 0.01574382558465004, 0.026622110977768898, 0.014198348857462406, 0.010745667852461338], 'this': [0.013838253915309906, 0.023839132860302925, 0.024396350607275963, 0.014796817675232887, 0.025020768865942955, 0.013344303704798222, 0.010099304839968681], 'exact': [0.06308653205633163, 0.10867902636528015, 0.11121930181980133, 0.06745648384094238, 0.11406593024730682, 0.06083468720316887, 0.04604122042655945], 'same': [0.05160975083708763, 0.08890800178050995, 0.09098615497350693, 0.05518471822142601, 0.09331491589546204, 0.04976756498217583, 0.03766534477472305], 'movie': [0.01584354229271412, 0.019353017210960388, 0.051392052322626114, 0.02729363739490509, 0.019693544134497643, 0.02037416771054268, 0.02196677401661873, 0.06020256504416466, 0.02793160267174244, 0.0675201490521431, 0.01694101095199585, 0.06340780109167099, 0.02156580612063408, 0.04268220439553261, 0.028646504506468773, 0.015278014354407787, 0.020024670287966728, 0.011562786065042019], 'hundred': [0.05462837591767311, 0.09410817921161652, 0.09630788117647171, 0.05841244012117386, 0.09877285361289978, 0.05267844349145889, 0.039868369698524475], 'times': [0.013873053714632988, 0.023899078369140625, 0.024457698687911034, 0.014834028668701649, 0.025083687156438828, 0.013377861119806767, 0.010124702006578445], 'unbelievably': [0.1970137357711792, 0.3875645697116852, 0.4922221899032593, 0.14555475115776062], 'film': [0.058822695165872574, 0.11674796789884567, 0.11571574956178665, 0.12755894660949707, 0.03193696215748787, 0.05262140929698944, 0.04345850646495819, 0.07412328571081161], 'though': [0.052930254489183426, 0.039105143398046494], 'occasionally': [0.05094499513506889, 0.03763842210173607], 'fun': [0.12189542502164841, 0.09005695581436157], 'enough': [0.05480239540338516, 0.040488287806510925], 'make': [0.04881848767399788, 0.03606734797358513], 'you': [0.0754670724272728, 0.2181040495634079, 0.05661127343773842, 0.06211565434932709, 0.04903833195567131, 0.04818420857191086, 0.054845571517944336, 0.04097379371523857, 0.05379107967019081, 0.055755455046892166, 0.04954645782709122], 'truly': [0.0773373693227768, 0.050587236881256104, 0.04881586879491806, 0.07324276119470596, 0.047062311321496964], 'turn': [0.19452115893363953, 0.24878139793872833, 0.05539928749203682, 0.04373597353696823, 0.04797482118010521], 'bottomlessly': [0.15607841312885284, 0.15061315894126892, 0.1452028453350067], 'cynical': [0.15324261784553528, 0.14787666499614716, 0.14256465435028076], 'derivative': [0.1254614144563675, 0.13707926869392395, 0.05654878914356232, 0.07965545356273651], 'horror': [0.18352952599525452, 0.20052453875541687, 0.08272162824869156, 0.1165228933095932], 'very': [0.09666524082422256, 0.08908047527074814], 'annoying': [0.41329944133758545, 0.4575263261795044, 0.3808702230453491], 'rare': [0.052486538887023926, 0.05341007187962532, 0.055255960673093796, 0.054308101534843445], 'takes': [0.01921912282705307, 0.019557293504476547, 0.020233208313584328, 0.021814795210957527, 0.021416600793600082, 0.01988612860441208], 'such': [0.01792619563639164, 0.01824161782860756, 0.018872061744332314, 0.020347250625491142, 0.019975844770669937, 0.018548330292105675], 'speedy': [0.05519700050354004, 0.05616822466254234, 0.05810943618416786, 0.06265173107385635, 0.06150812283158302, 0.05711263045668602], 'swan': [0.05988231673836708, 0.06093598157167435, 0.06304197013378143, 0.06796982884407043, 0.06672915071249008, 0.061960555613040924], 'dive': [0.05206771194934845, 0.05298387259244919, 0.054815035313367844, 0.05909981206059456, 0.05802103877067566, 0.05387473851442337], 'from': [0.017484698444604874, 0.01779235154390335, 0.018407268449664116, 0.019846124574542046, 0.019483866170048714, 0.018091510981321335, 0.010446526110172272], '``': [0.034585561603307724, 0.2316986620426178, 0.11621414124965668, 0.03519411385059357, 0.13775281608104706, 0.26093658804893494, 0.03641044721007347, 0.03925657644867897, 0.05955487862229347, 0.038540009409189224, 0.03578586503863335], 'promising': [0.024552689865231514, 0.024984708055853844, 0.02584819868206978, 0.02786869741976261, 0.02735999785363674, 0.025404799729585648], \"''\": [0.016725648194551468, 0.11204995214939117, 0.056201402097940445, 0.01701994612812996, 0.06661754846572876, 0.017608167603611946, 0.018984561786055565, 0.02880086377263069, 0.01863802783191204, 0.017306119203567505], 'interesting': [0.03506114333868027, 0.035678066313266754, 0.036911122500896454, 0.039796389639377594, 0.039069969207048416, 0.036277953535318375], 'familiar': [0.017370149493217468, 0.017675789073109627, 0.018286677077412605, 0.019716106355190277, 0.019356220960617065, 0.01797298714518547], 'before': [0.04653225839138031, 0.15635736286640167, 0.04735102131962776, 0.048987507820129395, 0.052816759794950485, 0.051852673292160034, 0.04814717918634415], 'landing': [0.07393749803304672, 0.24844422936439514, 0.07523847371339798, 0.29448992013931274, 0.07783876359462738, 0.08392325788736343, 0.08239137381315231, 0.07650352269411087], 'squarely': [0.027862660586833954, 0.09362390637397766, 0.028352919965982437, 0.11097578704357147, 0.02933281660079956, 0.03162570297718048, 0.031048424541950226, 0.02882964350283146], 'lame': [0.10793758183717728, 0.09574005752801895], 'romantic': [0.03487700596451759, 0.03093571960926056], 'comedy': [0.028175994753837585, 0.0249919556081295], 'unsympathetic': [0.10871434211730957, 0.13879366219043732, 0.09642903506755829, 0.13482406735420227], 'character': [0.029088633134961128, 0.03713693842291832, 0.025801461189985275, 0.036074794828891754], 'someone': [0.07161911576986313, 0.09143484383821487, 0.06352577358484268, 0.08881974220275879], 'who': [0.024032725021243095, 0.020929601043462753, 0.030682150274515152, 0.021316897124052048, 0.029804619029164314, 0.04586073383688927], 'likely': [0.02711271308362484, 0.03461431711912155, 0.024048831313848495, 0.033624324947595596, 0.051738157868385315, 0.054224953055381775], 'as': [0.03361475467681885, 0.06345772743225098, 0.042915355414152145, 0.02981610596179962, 0.041687946766614914, 0.05270301178097725, 0.06414575129747391, 0.06722892075777054], 'profoundly': [0.023385629057884216, 0.19929863512516022, 0.12071367353200912, 0.13440968096256256], 'affair': [0.056648049503564835, 0.29241010546684265, 0.3255865275859833], 'populating': [0.0877949669957161], 'its': [0.02051365375518799, 0.032784353941679], 'hackneyed': [0.07428223639726639], 'meanspirited': [0.09327492117881775], 'storyline': [0.027255015447735786], 'cardboard': [0.09595618396997452], 'performers': [0.02509729191660881], 'value': [0.03645777702331543], 'cash': [0.08785182982683182], 'above': [0.06449463963508606], 'credibility': [0.04755265638232231], 'opposed': [0.12700319290161133, 0.10547888278961182], 'manifesto': [0.08459842205047607, 0.07026080787181854], 'is': [0.043616726994514465, 0.09513171762228012, 0.03265092894434929, 0.02779056318104267, 0.031632546335458755, 0.023631907999515533, 0.02213932015001774, 0.04381820186972618, 0.03622463345527649, 0.04762475565075874, 0.10501551628112793, 0.03848210349678993, 0.040354713797569275, 0.016072507947683334, 0.04580384120345116, 0.01633220911026001, 0.02857624925673008, 0.009813402779400349], 'loud': [0.21378719806671143, 0.20344959199428558], 'silly': [0.1421556919813156, 0.13528180122375488, 0.1934070736169815], 'pointless': [0.24921101331710815, 0.29819509387016296, 0.4213656485080719, 0.4613042175769806, 0.23716050386428833, 0.24792571365833282, 0.2780609130859375, 0.26112547516822815, 0.46009865403175354, 0.4203595519065857, 0.3390590250492096], 'insanely': [0.0958949625492096, 0.2911260724067688, 0.06261330097913742, 0.220253124833107, 0.053292788565158844, 0.06066039949655533, 0.045317910611629486, 0.24873612821102142, 0.0913279727101326, 0.1350342482328415, 0.05479946732521057], 'awful': [0.21863201260566711, 0.1427527815103531, 0.1215028315782547, 0.1383003294467926, 0.10332081466913223, 0.20821969211101532, 0.30786609649658203, 0.12493792176246643], 'in': [0.05683761462569237, 0.03711134195327759, 0.01718207821249962, 0.04071972146630287, 0.03214692696928978, 0.03158700838685036, 0.03595384210348129, 0.02686024270951748, 0.0352625735104084, 0.05413072928786278, 0.03248002752661705], 'many': [0.05761442333459854, 0.037618547677993774, 0.032018713653087616, 0.03644523024559021, 0.02722734585404396, 0.054870542138814926, 0.03292393684387207], 'ways': [0.11925951391458511, 0.07786886394023895, 0.06627742946147919, 0.07544013857841492, 0.05635949596762657, 0.11357978731393814, 0.06815120577812195], 'pretty': [0.254334419965744, 0.18178972601890564, 0.29319313168525696, 0.20055650174617767, 0.22509288787841797, 0.13317431509494781], 'ugly': [0.2923126518726349, 0.24303492903709412, 0.27257564663887024, 0.25597429275512695], 'nothing': [0.05386240780353546, 0.06507399678230286, 0.06612546741962433, 0.039732273668050766], 'watching': [0.05897924676537514, 0.05019969493150711, 0.05713969096541405, 0.04268767684698105, 0.051618922501802444], 'leaves': [0.036860186606645584, 0.03137324005365372, 0.035710521042346954, 0.02667846344411373, 0.032260213047266006], 'giddy': [0.09539449214935303, 0.08119422197341919, 0.09241914749145508, 0.0690440982580185, 0.08348972350358963], 'sinks': [0.07217875123023987], 'low': [0.06104726344347], 'poorly': [0.07244303077459335], 'one': [0.03760165721178055, 0.029685311019420624, 0.03256238251924515], 'look': [0.05601808801293373, 0.04422449693083763, 0.04851069301366806], 'at': [0.03547516092658043, 0.028006508946418762, 0.030720870941877365], 'girl': [0.054648034274578094, 0.043142881244421005, 0.047324247658252716], 'tight': [0.08049538731575012, 0.06354854255914688, 0.06970760226249695], 'pants': [0.11926038563251495, 0.09415227919816971, 0.10327742248773575], 'big': [0.04544973373413086, 0.03588111698627472, 0.039358679205179214], 'tits': [0.13697485625743866, 0.10813728719949722, 0.11861784011125565], 'far-flung': [0.2196093052625656, 0.200466126203537, 0.1760537475347519, 0.18462084233760834, 0.20955035090446472], 'illogical': [0.22437503933906555, 0.20481644570827484, 0.17987428605556488, 0.18862730264663696, 0.214097797870636], 'plain': [0.18697476387023926, 0.1706763207912445, 0.1498916894197464, 0.15718568861484528, 0.178410604596138], '?': [0.1221761628985405, 0.13401734828948975], 'comes': [0.050283633172512054, 0.04275905340909958, 0.05170523747801781], 'along': [0.04367862641811371, 0.03714244067668915, 0.04491349682211876], 'every': [0.03307631239295006], 'often': [0.034916460514068604], 'substitutes': [0.1684524565935135, 0.2372846007347107], 'extreme': [0.05234556272625923, 0.07373473048210144], 'gore': [0.16451744735240936], 'for': [0.04400074854493141], 'suspense': [0.04304209351539612], 'there': [0.03802678361535072], 'difference': [0.037559621036052704], 'between': [0.03786900267004967], 'involved': [0.04276585206389427], 'her': [0.02601654827594757], '-lrb-': [0.0616142675280571, 0.06641384214162827], '-rrb-': [0.06618798524141312, 0.07134383171796799], 'soulless': [0.2845000624656677, 0.30666178464889526], 'sequel': [0.11704432964324951, 0.29490888118743896, 0.12616173923015594], '...': [0.07226764410734177, 0.04640384390950203], 'equilibrium': [0.16947832703590393], 'forget': [0.07538030296564102], 'absurdity': [0.12121577560901642], 'infantile': [0.185705304145813, 0.17694972455501556, 0.11996465176343918], 'redundant': [0.23895590007305145, 0.2276896834373474, 0.15436425805091858], 'sloppy': [0.1951589584350586, 0.1859576553106308, 0.12607166171073914], 'story': [0.03805799409747124, 0.0399099662899971], 'i': [0.017734987661242485, 0.01802155189216137], \"'m\": [0.04146707430481911, 0.04213710501790047], 'sure': [0.03419025242328644, 0.034742701798677444], 'which': [0.022912101820111275, 0.02328231744468212], 'half': [0.04053026810288429, 0.0411851592361927], 'dragonfly': [0.033127009868621826, 0.033662278205156326], 'worse': [0.0732235461473465, 0.074406698346138], ':': [0.018762001767754555, 0.019065160304307938], 'part': [0.033321771770715714, 0.033860187977552414], 'where': [0.03432035818696022, 0.03487490862607956], 'happening': [0.0381360724568367, 0.038752276450395584], 'or': [0.04668588191270828, 0.04744023457169533], 'something': [0.02586698718369007, 0.026284947991371155], 'frankly': [0.2674265503883362], 'script': [0.023120837286114693, 0.017498429864645004], 'over-the-top': [0.0802847221493721], 'amateurish': [0.1452258825302124], 'boilerplate': [0.04359188675880432], 'clichés': [0.040642257779836655], 'start': [0.012468530796468258], 'finish': [0.016479477286338806]}"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## 取出含有‘stupid’的句子，并计算句子中每个单词的attention\n",
    "stupid_sents = []\n",
    "stupid_words = []\n",
    "for sentence in train_sentences:\n",
    "    if 'stupid' in sentence:\n",
    "        stupid_sents.append(sentence)\n",
    "        for word in sentence:\n",
    "            stupid_words.append(word)\n",
    "\n",
    "\n",
    "stupid_word_attention = {w:[] for w in stupid_words}\n",
    "for s in stupid_sents:\n",
    "    s_w_attn = get_attention(s, word_embedding, u, word2dix)\n",
    "    for w in s_w_attn:\n",
    "        if w in stupid_word_attention:\n",
    "            stupid_word_attention[w].append(s_w_attn[w])\n",
    "print(stupid_word_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_std_stupid, sorted_mean_stupid= mean_std_list(stupid_word_attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 以‘stupid’的attention从大到小排序，可以看出来 attention 随着句子的长度和句子中出现重要单词的多少而减小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 'stupid'),\n",
       " (1.0, 'stupid'),\n",
       " (0.8240495920181274, 'stupid ,'),\n",
       " (0.8240495920181274, ', stupid'),\n",
       " (0.8237711787223816, 'stupid characters'),\n",
       " (0.8222578763961792, \"'s stupid\"),\n",
       " (0.8200607299804688, 'stupid and'),\n",
       " (0.8061841130256653, 'so stupid'),\n",
       " (0.800701379776001, 'profoundly stupid'),\n",
       " (0.7876837849617004, 'stupid americans'),\n",
       " (0.78346186876297, 'being stupid'),\n",
       " (0.7657994031906128, 'really stupid'),\n",
       " (0.7512186169624329, 'turn stupid'),\n",
       " (0.7088738679885864, 'insanely stupid'),\n",
       " (0.7068068981170654, 'pretty stupid'),\n",
       " (0.7050910592079163, 'stupid sequel'),\n",
       " (0.6936635375022888, \"it 's stupid\"),\n",
       " (0.6581787467002869, ', really stupid'),\n",
       " (0.6335198283195496, 'simply stupid ,'),\n",
       " (0.6308980584144592, 'be so stupid'),\n",
       " (0.6131293773651123, \"'s pretty stupid\"),\n",
       " (0.6056569814682007, 'so insanely stupid'),\n",
       " (0.587374746799469, 'you turn stupid'),\n",
       " (0.5786352157592773, 'on `` stupid'),\n",
       " (0.5478944182395935, 'really , really stupid'),\n",
       " (0.5426362156867981, \"'s pretty stupid .\"),\n",
       " (0.540003776550293, 'profoundly stupid affair'),\n",
       " (0.5363026261329651, 'so insanely stupid ,'),\n",
       " (0.5137991905212402, \"on `` stupid ''\"),\n",
       " (0.5077778100967407, 'unbelievably stupid'),\n",
       " (0.49585697054862976, 'too stupid'),\n",
       " (0.49035701155662537, 'is really , really stupid'),\n",
       " (0.4849787652492523, 'a profoundly stupid affair'),\n",
       " (0.4834858477115631, \"it 's pretty stupid .\"),\n",
       " (0.44486135244369507, 'stupid and annoying'),\n",
       " (0.4442058503627777, 'is really , really stupid .'),\n",
       " (0.44391199946403503, 'pointless , stupid'),\n",
       " (0.4427518844604492, 'stupid and pointless'),\n",
       " (0.4407324492931366, 'stupid , derivative horror film'),\n",
       " (0.43824437260627747, \", it 's pretty stupid .\"),\n",
       " (0.40547922253608704, ', pointless , stupid'),\n",
       " (0.404511034488678, ', stupid and pointless'),\n",
       " (0.4033791422843933, 'a stupid , derivative horror film'),\n",
       " (0.40185871720314026, 'very stupid and annoying'),\n",
       " (0.3998126983642578, 'an unbelievably stupid film'),\n",
       " (0.3887804448604584, \"'s also too stupid\"),\n",
       " (0.38554927706718445, 'simply stupid , irrelevant'),\n",
       " (0.3703272044658661, 'very stupid and annoying .'),\n",
       " (0.3554767072200775, 'simply stupid , irrelevant and'),\n",
       " (0.3287999629974365, 'so insanely stupid , so awful'),\n",
       " (0.3262757360935211, 'silly , stupid and pointless'),\n",
       " (0.323175847530365, 'simply stupid , irrelevant and deeply'),\n",
       " (0.3210461437702179, \"frankly , it 's pretty stupid .\"),\n",
       " (0.305471271276474, \"landing squarely on `` stupid ''\"),\n",
       " (0.3020733594894409, \"'s simply stupid , irrelevant and deeply\"),\n",
       " (0.3016372621059418, '-lrb- a -rrb- soulless , stupid sequel'),\n",
       " (0.2869524657726288, 'ugly , pointless , stupid'),\n",
       " (0.2837705910205841, \"'s simply stupid , irrelevant and deeply ,\"),\n",
       " (0.2798386514186859, '-lrb- a -rrb- soulless , stupid sequel ...'),\n",
       " (0.2683190107345581, \"do n't care about being stupid\"),\n",
       " (0.2675773799419403, 'ugly , pointless , stupid movie'),\n",
       " (0.26182451844215393, \"'s simply stupid , irrelevant and deeply , truly\"),\n",
       " (0.2577085793018341, \"before landing squarely on `` stupid ''\"),\n",
       " (0.2561054229736328,\n",
       "  'a stupid , derivative horror film that substitutes extreme'),\n",
       " (0.25393128395080566, \"that do n't care about being stupid\"),\n",
       " (0.25128045678138733, 'an ugly , pointless , stupid movie'),\n",
       " (0.24796229600906372, \"'s simply stupid , irrelevant and deeply , truly ,\"),\n",
       " (0.23981519043445587, 'loud , silly , stupid and pointless'),\n",
       " (0.23857834935188293, 'an ugly , pointless , stupid movie .'),\n",
       " (0.23349826037883759, 'so insanely stupid , so awful in so many ways'),\n",
       " (0.23173819482326508, 'stupid , infantile , redundant , sloppy'),\n",
       " (0.22821901738643646, 'loud , silly , stupid and pointless .'),\n",
       " (0.22414197027683258, 'far-flung , illogical , and plain stupid'),\n",
       " (0.2223779261112213, 'is so insanely stupid , so awful in so many ways'),\n",
       " (0.22081227600574493, 'stupid , infantile , redundant , sloppy ,'),\n",
       " (0.2157662957906723, 'would not likely be so stupid as to get'),\n",
       " (0.213875412940979, 'is far-flung , illogical , and plain stupid'),\n",
       " (0.2058711051940918, 'who would not likely be so stupid as to get'),\n",
       " (0.20460370182991028, 'is far-flung , illogical , and plain stupid .'),\n",
       " (0.2036629319190979,\n",
       "  'the movie , as opposed to the manifesto , is really , really stupid .'),\n",
       " (0.20323993265628815,\n",
       "  'an unbelievably stupid film , though occasionally fun enough to make you'),\n",
       " (0.18843136727809906,\n",
       "  'the story is far-flung , illogical , and plain stupid .'),\n",
       " (0.18570968508720398,\n",
       "  'absurd plot twists , idiotic court maneuvers and stupid characters'),\n",
       " (0.18550170958042145,\n",
       "  \"the top and movies that do n't care about being stupid\"),\n",
       " (0.18181368708610535,\n",
       "  'a stupid , derivative horror film that substitutes extreme gore for suspense .'),\n",
       " (0.17968744039535522,\n",
       "  '... the story is far-flung , illogical , and plain stupid .'),\n",
       " (0.1783759444952011,\n",
       "  'of absurd plot twists , idiotic court maneuvers and stupid characters'),\n",
       " (0.17126235365867615,\n",
       "  \"'s simply stupid , irrelevant and deeply , truly , bottomlessly cynical\"),\n",
       " (0.16914644837379456,\n",
       "  'equilibrium the movie , as opposed to the manifesto , is really , really stupid .'),\n",
       " (0.16876907646656036,\n",
       "  'game of absurd plot twists , idiotic court maneuvers and stupid characters'),\n",
       " (0.1672833114862442,\n",
       "  'one look at a girl in tight pants and big tits and you turn stupid'),\n",
       " (0.16526541113853455,\n",
       "  \"'s simply stupid , irrelevant and deeply , truly , bottomlessly cynical .\"),\n",
       " (0.15932875871658325,\n",
       "  \"it 's simply stupid , irrelevant and deeply , truly , bottomlessly cynical .\"),\n",
       " (0.15484601259231567,\n",
       "  \"go over the top and movies that do n't care about being stupid\"),\n",
       " (0.15245948731899261,\n",
       "  'is so insanely stupid , so awful in so many ways that watching it leaves you giddy'),\n",
       " (0.1520555019378662,\n",
       "  'played game of absurd plot twists , idiotic court maneuvers and stupid characters'),\n",
       " (0.1501546949148178,\n",
       "  'an unbelievably stupid film , though occasionally fun enough to make you forget its absurdity .'),\n",
       " (0.14970165491104126,\n",
       "  'stupid , infantile , redundant , sloppy , over-the-top , and amateurish .'),\n",
       " (0.14770428836345673,\n",
       "  'that is so insanely stupid , so awful in so many ways that watching it leaves you giddy'),\n",
       " (0.14486443996429443,\n",
       "  'one look at a girl in tight pants and big tits and you turn stupid ?'),\n",
       " (0.13773369789123535,\n",
       "  'an unsympathetic character and someone who would not likely be so stupid as to get'),\n",
       " (0.1337944120168686,\n",
       "  'about an unsympathetic character and someone who would not likely be so stupid as to get'),\n",
       " (0.1334332823753357,\n",
       "  'comes along that is so insanely stupid , so awful in so many ways that watching it leaves you giddy'),\n",
       " (0.1320648491382599,\n",
       "  \"`` one look at a girl in tight pants and big tits and you turn stupid ? ''\"),\n",
       " (0.13114982843399048,\n",
       "  \"the courage to go over the top and movies that do n't care about being stupid\"),\n",
       " (0.1297646164894104,\n",
       "  'comes along that is so insanely stupid , so awful in so many ways that watching it leaves you giddy .'),\n",
       " (0.12267553061246872,\n",
       "  \"movies with the courage to go over the top and movies that do n't care about being stupid\"),\n",
       " (0.11352398991584778,\n",
       "  \"'s also too stupid to realize that they 've already seen this exact same movie a hundred times\"),\n",
       " (0.1106908842921257,\n",
       "  \"it 's also too stupid to realize that they 've already seen this exact same movie a hundred times\"),\n",
       " (0.11034628748893738,\n",
       "  'every so often a film comes along that is so insanely stupid , so awful in so many ways that watching it leaves you giddy .'),\n",
       " (0.10816267877817154,\n",
       "  \"that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times\"),\n",
       " (0.10788409411907196,\n",
       "  'a lame romantic comedy about an unsympathetic character and someone who would not likely be so stupid as to get'),\n",
       " (0.10759475827217102,\n",
       "  'stupid americans will get a kick out of goofy brits with cute accents performing ages-old slapstick and unfunny tricks'),\n",
       " (0.10367858409881592,\n",
       "  'if stupid americans will get a kick out of goofy brits with cute accents performing ages-old slapstick and unfunny tricks'),\n",
       " (0.1033768281340599,\n",
       "  \"there is a difference between movies with the courage to go over the top and movies that do n't care about being stupid\"),\n",
       " (0.09753934293985367,\n",
       "  'to see if stupid americans will get a kick out of goofy brits with cute accents performing ages-old slapstick and unfunny tricks'),\n",
       " (0.09664229303598404,\n",
       "  \"played game of absurd plot twists , idiotic court maneuvers and stupid characters that even freeman ca n't save it\"),\n",
       " (0.09569261223077774,\n",
       "  'a lame romantic comedy about an unsympathetic character and someone who would not likely be so stupid as to get involved with her .'),\n",
       " (0.09470311552286148,\n",
       "  \"played game of absurd plot twists , idiotic court maneuvers and stupid characters that even freeman ca n't save it .\"),\n",
       " (0.09395401179790497,\n",
       "  'a profoundly stupid affair , populating its hackneyed and meanspirited storyline with cardboard characters and performers who value cash above credibility .'),\n",
       " (0.08705271035432816,\n",
       "  \"see a movie that takes such a speedy swan dive from `` promising '' to `` interesting '' to `` familiar '' before landing squarely on `` stupid ''\"),\n",
       " (0.08546370267868042,\n",
       "  \"to see a movie that takes such a speedy swan dive from `` promising '' to `` interesting '' to `` familiar '' before landing squarely on `` stupid ''\"),\n",
       " (0.08074133098125458,\n",
       "  \"rare to see a movie that takes such a speedy swan dive from `` promising '' to `` interesting '' to `` familiar '' before landing squarely on `` stupid ''\"),\n",
       " (0.07935629785060883,\n",
       "  \"'s rare to see a movie that takes such a speedy swan dive from `` promising '' to `` interesting '' to `` familiar '' before landing squarely on `` stupid ''\"),\n",
       " (0.07804407179355621,\n",
       "  \"'s rare to see a movie that takes such a speedy swan dive from `` promising '' to `` interesting '' to `` familiar '' before landing squarely on `` stupid '' .\"),\n",
       " (0.07669458538293839,\n",
       "  \"it 's rare to see a movie that takes such a speedy swan dive from `` promising '' to `` interesting '' to `` familiar '' before landing squarely on `` stupid '' .\"),\n",
       " (0.0762612372636795,\n",
       "  \"i 'm not sure which half of dragonfly is worse : the part where nothing 's happening , or the part where something 's happening , but it 's stupid\"),\n",
       " (0.07504859566688538,\n",
       "  \"i 'm not sure which half of dragonfly is worse : the part where nothing 's happening , or the part where something 's happening , but it 's stupid .\"),\n",
       " (0.0744263157248497,\n",
       "  'cranked out on an assembly line to see if stupid americans will get a kick out of goofy brits with cute accents performing ages-old slapstick and unfunny tricks'),\n",
       " (0.07058679312467575,\n",
       "  \"sinks so low in a poorly played game of absurd plot twists , idiotic court maneuvers and stupid characters that even freeman ca n't save it .\"),\n",
       " (0.06713598966598511,\n",
       "  \"that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times\"),\n",
       " (0.06658100336790085,\n",
       "  'more than a widget cranked out on an assembly line to see if stupid americans will get a kick out of goofy brits with cute accents performing ages-old slapstick and unfunny tricks'),\n",
       " (0.06565481424331665,\n",
       "  'more than a widget cranked out on an assembly line to see if stupid americans will get a kick out of goofy brits with cute accents performing ages-old slapstick and unfunny tricks .'),\n",
       " (0.06278679519891739,\n",
       "  \"assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times\"),\n",
       " (0.06211848556995392,\n",
       "  'nothing more than a widget cranked out on an assembly line to see if stupid americans will get a kick out of goofy brits with cute accents performing ages-old slapstick and unfunny tricks .'),\n",
       " (0.060545653104782104,\n",
       "  \"the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times\"),\n",
       " (0.045822471380233765,\n",
       "  \"the plot is nothing but boilerplate clichés from start to finish , and the script assumes that not only would subtlety be lost on the target audience , but that it 's also too stupid to realize that they 've already seen this exact same movie a hundred times\")]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_atten = []\n",
    "for i in range(len(stupid_sents)):\n",
    "    sent_atten.append((stupid_word_attention['stupid'][i],  \" \".join(stupid_sents[i])))\n",
    "    #print('attention:', stupid_word_attention['stupid'][i],  \" \".join(stupid_sents[i]), '\\n')\n",
    "    \n",
    "## 以‘stupid’的attention从大到小排序\n",
    "sorted(sent_atten, key=lambda x:x[0], reverse=True) \n",
    "## 可以看出来 attention 随着句子的长度和句子中出现重要单词的多少而减小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
