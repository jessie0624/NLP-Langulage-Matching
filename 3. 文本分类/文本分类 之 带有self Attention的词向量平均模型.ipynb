{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本分类 之 self attention 机制\n",
    "\n",
    "在前面 word average model 和 word average with attention model的基础上，我们做个扩展，加上self attention.\n",
    "\n",
    "我们再定义一种基于self-attention 的句子模型。\n",
    "\n",
    "$$\\alpha_t = emb(x_t)^T emb(x_s)$$\n",
    "$$\\alpha_t \\propto exp\\{\\sum_t\\alpha_{ts}\\}$$\n",
    "$$h_{self} = \\sum_t\\alpha_t emb(x_t)$$\n",
    "\n",
    "句子的正面情感的概率为\n",
    "$$\\sigma(W^Th_{self})$$\n",
    "单词的权重是该单词的embedding和所有其他单词的embedding的dot product的和，然后做softmax归一化。这个模型和 word average with attention 的区别是没有额外引入模型参数u.\n",
    "\n",
    "另一个变种是把词向量的平均向量也加入self-attention向量，相当于一种ersidual connection 的方法。\n",
    "$$\\sigma(W^T(h_{self} + h_{avg}))$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hide new secretions from the parental units\\t0\\n', 'contains no wit , only labored gags\\t0\\n', 'that loves its characters and communicates something rather beautiful about human nature\\t1\\n', 'remains utterly satisfied to remain the same throughout\\t0\\n', 'on the worst revenge-of-the-nerds clich茅s the filmmakers could dredge up\\t0\\n', \"that 's far too tragic to merit such superficial treatment\\t0\\n\", 'demonstrates that the director of such Hollywood blockbusters as Patriot Games can still turn out a small , personal film with an emotional wallop .\\t1\\n', 'of saucy\\t1\\n', \"a depressed fifteen-year-old 's suicidal poetry\\t0\\n\", \"are more deeply thought through than in most ` right-thinking ' films\\t1\\n\"]\n"
     ]
    }
   ],
   "source": [
    "with open('senti.train.tsv','r') as rf:\n",
    "    lines = rf.readlines()\n",
    "print(lines[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(path,'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            sentence, label = line.split('\\t')\n",
    "            sentences.append(sentence.lower().split())\n",
    "            labels.append(label[0])\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path,dev_path,test_path = 'senti.train.tsv','senti.dev.tsv','senti.test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences, train_labels = read_corpus(train_path)\n",
    "dev_sentences, dev_labels = read_corpus(dev_path)\n",
    "test_sentences, test_labels = read_corpus(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['contains', 'no', 'wit', ',', 'only', 'labored', 'gags'], '0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[1], train_labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(sentences, word_size=20000):\n",
    "    c = Counter()\n",
    "    for sent in sentences:\n",
    "        for word in sent:\n",
    "            c[word] += 1\n",
    "    print('文本总单词量为：',len(c))\n",
    "    words_most_common = c.most_common(word_size)\n",
    "    ## adding unk, pad\n",
    "    idx2word = ['<pad>','<unk>'] + [item[0] for item in words_most_common]\n",
    "    word2dix = {w:i for i, w in enumerate(idx2word)}\n",
    "    return idx2word, word2dix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本总单词量为： 14828\n"
     ]
    }
   ],
   "source": [
    "WORD_SIZE=20000\n",
    "idx2word, word2dix = build_vocab(train_sentences, word_size=WORD_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<unk>', 'the', ',', 'a', 'and', 'of', '.', 'to', \"'s\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numeralization(sentences, labels, word2idx):\n",
    "    '把word list表示的句子转成 index 表示的列表'\n",
    "    numeral_sent = [[word2dix.get(w, word2dix['<unk>']) for w in s] for s in sentences]\n",
    "    numeral_label =[int(label) for label in labels]\n",
    "    return list(zip(numeral_sent, numeral_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train_data = numeralization(train_sentences, train_labels, word2dix)\n",
    "num_test_data = numeralization(test_sentences, test_labels, word2dix)\n",
    "num_dev_data = numeralization(dev_sentences, dev_labels, word2dix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert2tensor(batch_sentences):\n",
    "    '将batch数据转成tensor,这里主要是为了padding'\n",
    "    lengths = [len(s) for s in batch_sentences]\n",
    "    max_len = max(lengths)\n",
    "    batch_size = len(batch_sentences)\n",
    "    batch = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
    "    for i, l in enumerate(lengths):\n",
    "        batch[i, :l] = torch.tensor(batch_sentences[i])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(numeral_sentences_labels, batch_size=32):\n",
    "    '''将list index 数据 分成batch '''\n",
    "    batches = []\n",
    "    num_sample = len(numeral_sentences_labels)\n",
    "    random.shuffle(numeral_sentences_labels)\n",
    "    numeral_sent = [n[0] for n in numeral_sentences_labels]\n",
    "    numeral_label = [n[1] for n in numeral_sentences_labels]\n",
    "    for start in range(0, num_sample, batch_size):\n",
    "        end = start + batch_size\n",
    "        if end > num_sample:\n",
    "            batch_sentences = numeral_sent[start : num_sample]\n",
    "            batch_labels = numeral_label[start : num_sample]\n",
    "            batch_sent_tensor = convert2tensor(batch_sentences)\n",
    "            batch_label_tensor = torch.tensor(batch_labels, dtype=torch.float)\n",
    "        else:\n",
    "            batch_sentences = numeral_sent[start : end]\n",
    "            batch_labels = numeral_label[start : end]\n",
    "            batch_sent_tensor = convert2tensor(batch_sentences)\n",
    "            batch_label_tensor = torch.tensor(batch_labels, dtype=torch.float)\n",
    "        batches.append((batch_sent_tensor.cuda(), batch_label_tensor.cuda()))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = generate_batch(num_train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AVGSelfAttnModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, output_size, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.fc = nn.Linear(embed_dim, output_size,bias=False)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        ## [batch_size, seq_len]->[batch_size, seq_len, embed_dim]\n",
    "        embed = self.embedding(text)\n",
    "        x = self.qkv(embed) \n",
    "        ## 计算句子attention\n",
    "        h_attn = self.attention(x)\n",
    "        ## 计算平 整个句子 attention 之后的embedding 句子相加得到句子的表示\n",
    "        h_attn = torch.sum(h_attn, dim=1).squeeze()\n",
    "        out = self.fc(h_attn)\n",
    "        return out\n",
    "    \n",
    "    def attention(self, x):\n",
    "        d_k = x.size(-1)\n",
    "        ##[batch_size, seq_len, embed_dim] ->[batch_size, seq_len, seq_len]\n",
    "        score = torch.matmul(x, x.transpose(-2, -1))/math.sqrt(d_k)\n",
    "        ## 计算权重 attn:[batch_size, seq_len, seq_len]\n",
    "        attn = F.softmax(score, dim=-1)\n",
    "        ## 计算context 值 attn_x: [batch_size, seq_len, embed_dim]\n",
    "        attn_x = torch.matmul(attn, x)\n",
    "        return attn_x\n",
    "\n",
    "    def get_embed_weigth(self):\n",
    "        return self.embedding.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word2dix)\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_SIZE = 1\n",
    "PAD_IDX = word2dix['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AVGSelfAttnModel(\n",
       "  (embedding): Embedding(14830, 100, padding_idx=0)\n",
       "  (qkv): Linear(in_features=100, out_features=100, bias=False)\n",
       "  (fc): Linear(in_features=100, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AVGSelfAttnModel(vocab_size=VOCAB_SIZE,\n",
    "                 embed_dim=EMBEDDING_DIM,\n",
    "                 output_size=OUTPUT_SIZE, \n",
    "                 pad_idx=PAD_IDX)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数 和优化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(output, label):\n",
    "    ## output: batch_size \n",
    "    y_hat = torch.round(torch.sigmoid(output)) ## 将output 转成0和1\n",
    "    correct = (y_hat == label).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(batch_data, model, criterion, get_accuracy):\n",
    "    model.eval()\n",
    "    num_epoch = epoch_loss = epoch_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for text, label in batch_data:\n",
    "            out = model(text).squeeze(1)\n",
    "            loss = criterion(out, label)\n",
    "            acc = get_accuracy(out, label)\n",
    "            num_epoch +=1 \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss/num_epoch, epoch_acc/num_epoch          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(batch_data, model, criterion, optimizer, get_accuracy):\n",
    "    model.train()\n",
    "    num_epoch = epoch_loss = epoch_acc = 0\n",
    "    for text, label in batch_data:\n",
    "        model.zero_grad()\n",
    "        out = model(text).squeeze(1)\n",
    "        loss = criterion(out, label)\n",
    "        acc = get_accuracy(out, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        num_epoch +=1 \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss/num_epoch, epoch_acc/num_epoch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 :\n",
      "\t Train Loss: 0.5429 | Train Acc: 72.38%\n",
      "\t Valid Loss: 0.4695 | Valid Acc: 78.12%\n",
      "Epoch: 02 :\n",
      "\t Train Loss: 0.2947 | Train Acc: 88.60%\n",
      "\t Valid Loss: 0.5573 | Valid Acc: 79.02%\n",
      "Epoch: 03 :\n",
      "\t Train Loss: 0.2277 | Train Acc: 91.26%\n",
      "\t Valid Loss: 0.6375 | Valid Acc: 79.80%\n",
      "Epoch: 04 :\n",
      "\t Train Loss: 0.1964 | Train Acc: 92.50%\n",
      "\t Valid Loss: 0.7260 | Valid Acc: 80.25%\n",
      "Epoch: 05 :\n",
      "\t Train Loss: 0.1759 | Train Acc: 93.27%\n",
      "\t Valid Loss: 0.7696 | Valid Acc: 82.25%\n",
      "Epoch: 06 :\n",
      "\t Train Loss: 0.1642 | Train Acc: 93.81%\n",
      "\t Valid Loss: 0.8865 | Valid Acc: 80.58%\n",
      "Epoch: 07 :\n",
      "\t Train Loss: 0.1538 | Train Acc: 94.13%\n",
      "\t Valid Loss: 0.9686 | Valid Acc: 79.35%\n",
      "Epoch: 08 :\n",
      "\t Train Loss: 0.1461 | Train Acc: 94.53%\n",
      "\t Valid Loss: 0.9697 | Valid Acc: 81.81%\n",
      "Epoch: 09 :\n",
      "\t Train Loss: 0.1409 | Train Acc: 94.63%\n",
      "\t Valid Loss: 1.1235 | Valid Acc: 79.46%\n",
      "Epoch: 10 :\n",
      "\t Train Loss: 0.1356 | Train Acc: 94.89%\n",
      "\t Valid Loss: 1.1045 | Valid Acc: 81.14%\n",
      "Epoch: 11 :\n",
      "\t Train Loss: 0.1326 | Train Acc: 95.05%\n",
      "\t Valid Loss: 1.2394 | Valid Acc: 80.13%\n",
      "Epoch: 12 :\n",
      "\t Train Loss: 0.1296 | Train Acc: 95.11%\n",
      "\t Valid Loss: 1.3044 | Valid Acc: 79.35%\n",
      "Epoch: 13 :\n",
      "\t Train Loss: 0.1265 | Train Acc: 95.18%\n",
      "\t Valid Loss: 1.4154 | Valid Acc: 79.02%\n",
      "Epoch: 14 :\n",
      "\t Train Loss: 0.1242 | Train Acc: 95.28%\n",
      "\t Valid Loss: 1.4540 | Valid Acc: 79.35%\n",
      "Epoch: 15 :\n",
      "\t Train Loss: 0.1219 | Train Acc: 95.36%\n",
      "\t Valid Loss: 1.5596 | Valid Acc: 78.91%\n",
      "Epoch: 16 :\n",
      "\t Train Loss: 0.1208 | Train Acc: 95.40%\n",
      "\t Valid Loss: 1.5866 | Valid Acc: 78.68%\n",
      "Epoch: 17 :\n",
      "\t Train Loss: 0.1190 | Train Acc: 95.48%\n",
      "\t Valid Loss: 1.6453 | Valid Acc: 78.35%\n",
      "Epoch: 18 :\n",
      "\t Train Loss: 0.1175 | Train Acc: 95.51%\n",
      "\t Valid Loss: 1.6904 | Valid Acc: 79.35%\n",
      "Epoch: 19 :\n",
      "\t Train Loss: 0.1170 | Train Acc: 95.59%\n",
      "\t Valid Loss: 1.7406 | Valid Acc: 79.24%\n",
      "Epoch: 20 :\n",
      "\t Train Loss: 0.1160 | Train Acc: 95.57%\n",
      "\t Valid Loss: 1.8767 | Valid Acc: 77.01%\n",
      "Epoch: 21 :\n",
      "\t Train Loss: 0.1149 | Train Acc: 95.67%\n",
      "\t Valid Loss: 1.8612 | Valid Acc: 78.68%\n",
      "Epoch: 22 :\n",
      "\t Train Loss: 0.1142 | Train Acc: 95.62%\n",
      "\t Valid Loss: 1.9032 | Valid Acc: 78.46%\n",
      "Epoch: 23 :\n",
      "\t Train Loss: 0.1126 | Train Acc: 95.68%\n",
      "\t Valid Loss: 1.9864 | Valid Acc: 77.90%\n",
      "Epoch: 24 :\n",
      "\t Train Loss: 0.1118 | Train Acc: 95.78%\n",
      "\t Valid Loss: 2.0475 | Valid Acc: 76.67%\n",
      "Epoch: 25 :\n",
      "\t Train Loss: 0.1113 | Train Acc: 95.76%\n",
      "\t Valid Loss: 2.0904 | Valid Acc: 77.79%\n",
      "Epoch: 26 :\n",
      "\t Train Loss: 0.1100 | Train Acc: 95.85%\n",
      "\t Valid Loss: 2.1268 | Valid Acc: 77.01%\n",
      "Epoch: 27 :\n",
      "\t Train Loss: 0.1105 | Train Acc: 95.75%\n",
      "\t Valid Loss: 2.1717 | Valid Acc: 77.90%\n",
      "Epoch: 28 :\n",
      "\t Train Loss: 0.1092 | Train Acc: 95.88%\n",
      "\t Valid Loss: 2.2729 | Valid Acc: 77.46%\n",
      "Epoch: 29 :\n",
      "\t Train Loss: 0.1091 | Train Acc: 95.79%\n",
      "\t Valid Loss: 2.3031 | Valid Acc: 78.01%\n",
      "Epoch: 30 :\n",
      "\t Train Loss: 0.1082 | Train Acc: 95.95%\n",
      "\t Valid Loss: 2.3582 | Valid Acc: 77.34%\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCH = 30\n",
    "best_valid_acc = -1\n",
    "\n",
    "dev_data = generate_batch(num_dev_data)\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    train_data = generate_batch(num_train_data)\n",
    "    train_loss, train_acc = train(train_data, model, criterion, optimizer, get_accuracy)\n",
    "    valid_loss, valid_acc = evaluate(dev_data, model, criterion, get_accuracy)\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(model.state_dict(),'self-attn-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} :')\n",
    "    print(f'\\t Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc*100:.2f}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('self-attn-model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6522 |  Test Acc: 81.61%\n"
     ]
    }
   ],
   "source": [
    "test_data = generate_batch(num_test_data)\n",
    "test_loss, test_acc = evaluate(test_data, model, criterion, get_accuracy)\n",
    "print(f'Test Loss: {test_loss:.4f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分析词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_embedding = model.get_embed_weigth()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
