{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词向量平均模型\n",
    "我们用X = {x_1, x_2,x_3..x_n}表示一个句子，x_t是句子中的第t个单词，我们使用emb来表示单词的embedding函数，也就是说 emb(x)返回一个d维度的词向量。\n",
    "首先我们定义一个word_averaging 句子encoder:\n",
    "$$h_{avg} = 1/|x| * \\sum_t emb(x_t)$$\n",
    "然后，这个句子是正面情感的概率就是：\n",
    "$$pos = \\sigma(W^T * h_{avg})$$\n",
    "\n",
    "\\sigma是逻辑斯蒂函数， w 是一个d维向量。如果，pos>=0.5分类器就返回正面的情感，否则就返回负面情感.\n",
    "\n",
    "在训练的时候我们使用binary log loss。整个模型的参数就是embedding函数 emb 和向量 w 。注意词向量的维度 d 和 w 的维度必须相同。有些单词可能在DEV和TEST中出现，但是没有在TRAIN当中出现。针对这些单词，我们可以随机生成一个词向量(一个特殊的UNK词向量)。不过在初始化词向量的时候，注意不要初始化太大的范围，否则这些unknown words的norm太大可能会导致模型效果变差(所以这里我们将词向量初始化为-0.1到0.1之间的随机数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if USE_CUDA else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 读数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hide new secretions from the parental units\\t0\\n', 'contains no wit , only labored gags\\t0\\n', 'that loves its characters and communicates something rather beautiful about human nature\\t1\\n', 'remains utterly satisfied to remain the same throughout\\t0\\n', 'on the worst revenge-of-the-nerds clich茅s the filmmakers could dredge up\\t0\\n', \"that 's far too tragic to merit such superficial treatment\\t0\\n\", 'demonstrates that the director of such Hollywood blockbusters as Patriot Games can still turn out a small , personal film with an emotional wallop .\\t1\\n', 'of saucy\\t1\\n', \"a depressed fifteen-year-old 's suicidal poetry\\t0\\n\", \"are more deeply thought through than in most ` right-thinking ' films\\t1\\n\"]\n"
     ]
    }
   ],
   "source": [
    "with open('senti.train.tsv','r') as rf:\n",
    "    lines = rf.readlines()\n",
    "print(lines[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_corpus(path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    with open(path,'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            sentence, label = line.split('\\t')\n",
    "            sentences.append(sentence.lower().split())\n",
    "            labels.append(label[0])\n",
    "    return sentences, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path,dev_path,test_path = 'senti.train.tsv','senti.dev.tsv','senti.test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sentences, train_labels = read_corpus(train_path)\n",
    "dev_sentences, dev_labels = read_corpus(dev_path)\n",
    "test_sentences, test_labels = read_corpus(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67349\n",
      "67349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(train_sentences)), print(len(train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['contains', 'no', 'wit', ',', 'only', 'labored', 'gags'], '0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[1], train_labels[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(sentences, word_size=20000):\n",
    "    c = Counter()\n",
    "    for sent in sentences:\n",
    "        for word in sent:\n",
    "            c[word] += 1\n",
    "    print('文本总单词量为：',len(c))\n",
    "    words_most_common = c.most_common(word_size)\n",
    "    ## adding unk, pad\n",
    "    idx2word = ['<pad>','<unk>'] + [item[0] for item in words_most_common]\n",
    "    word2dix = {w:i for i, w in enumerate(idx2word)}\n",
    "    return idx2word, word2dix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文本总单词量为： 14828\n"
     ]
    }
   ],
   "source": [
    "WORD_SIZE=20000\n",
    "idx2word, word2dix = build_vocab(train_sentences, word_size=WORD_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<pad>', '<unk>', 'the', ',', 'a', 'and', 'of', '.', 'to', \"'s\"]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2word[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def numeralization(sentences, labels, word2idx):\n",
    "    '把word list表示的句子转成 index 表示的列表'\n",
    "    numeral_sent = [[word2dix.get(w, word2dix['<unk>']) for w in s] for s in sentences]\n",
    "    numeral_label =[int(label) for label in labels]\n",
    "    return list(zip(numeral_sent, numeral_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train_data = numeralization(train_sentences, train_labels, word2dix)\n",
    "num_test_data = numeralization(test_sentences, test_labels, word2dix)\n",
    "num_dev_data = numeralization(dev_sentences, dev_labels, word2dix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert2tensor(batch_sentences):\n",
    "    '将batch数据转成tensor,这里主要是为了padding'\n",
    "    lengths = [len(s) for s in batch_sentences]\n",
    "    max_len = max(lengths)\n",
    "    batch_size = len(batch_sentences)\n",
    "    batch = torch.zeros(batch_size, max_len, dtype=torch.long)\n",
    "    for i, l in enumerate(lengths):\n",
    "        batch[i, :l] = torch.tensor(batch_sentences[i])\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(numeral_sentences_labels, batch_size=32):\n",
    "    '''将list index 数据 分成batch '''\n",
    "    batches = []\n",
    "    num_sample = len(numeral_sentences_labels)\n",
    "    random.shuffle(numeral_sentences_labels)\n",
    "    numeral_sent = [n[0] for n in numeral_sentences_labels]\n",
    "    numeral_label = [n[1] for n in numeral_sentences_labels]\n",
    "    for start in range(0, num_sample, batch_size):\n",
    "        end = start + batch_size\n",
    "        if end > num_sample:\n",
    "            batch_sentences = numeral_sent[start : num_sample]\n",
    "            batch_labels = numeral_label[start : num_sample]\n",
    "            batch_sent_tensor = convert2tensor(batch_sentences)\n",
    "            batch_label_tensor = torch.tensor(batch_labels, dtype=torch.float)\n",
    "        else:\n",
    "            batch_sentences = numeral_sent[start : end]\n",
    "            batch_labels = numeral_label[start : end]\n",
    "            batch_sent_tensor = convert2tensor(batch_sentences)\n",
    "            batch_label_tensor = torch.tensor(batch_labels, dtype=torch.float)\n",
    "        batches.append((batch_sent_tensor.cuda(), batch_label_tensor.cuda()))\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = generate_batch(num_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = train_data[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text,label=a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    2,  1470,     0,  ...,     0,     0,     0],\n",
       "        [ 3789,     0,     0,  ...,     0,     0,     0],\n",
       "        [ 2056,    15,   283,  ...,     0,     0,     0],\n",
       "        ...,\n",
       "        [11711,     3, 12789,  ...,    42,  2365,     7],\n",
       "        [ 1484,   524,     0,  ...,     0,     0,     0],\n",
       "        [  308,    11,    10,  ...,     0,     0,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AVGModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, output_size, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        initrange = 0.1\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc = nn.Linear(embed_dim, output_size)\n",
    "    def forward(self, text):\n",
    "        ## [batch_size, seq_len]->[batch_size, seq_len, embed_dim]\n",
    "        embed = self.embedding(text)\n",
    "        ## attention\n",
    "        ##[batch_size, seq_len, embed_dim]->[batch_size, embed_dim]\n",
    "        pooled = F.avg_pool2d(embed, (embed.size(1),1)).squeeze(1)\n",
    "        ## [batch_size, embed_dim]->[batch_size, output_size]\n",
    "        out = self.fc(pooled)\n",
    "        return out\n",
    "    def get_embed_weigth(self):\n",
    "        return self.embedding.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(word2dix)\n",
    "EMBEDDING_DIM = 100\n",
    "OUTPUT_SIZE = 1\n",
    "PAD_IDX = word2dix['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AVGModel(\n",
       "  (embedding): Embedding(14830, 100, padding_idx=0)\n",
       "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AVGModel(vocab_size=VOCAB_SIZE,\n",
    "                 embed_dim=EMBEDDING_DIM,\n",
    "                 output_size=OUTPUT_SIZE, \n",
    "                 pad_idx=PAD_IDX)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义损失函数 和优化函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "criterion = criterion.to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_accuracy(output, label):\n",
    "    ## output: batch_size \n",
    "    y_hat = torch.round(torch.sigmoid(output)) ## 将output 转成0和1\n",
    "    correct = (y_hat == label).float()\n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(batch_data, model, criterion, get_accuracy):\n",
    "    model.eval()\n",
    "    num_epoch = epoch_loss = epoch_acc = 0\n",
    "    with torch.no_grad():\n",
    "        for text, label in batch_data:\n",
    "            out = model(text).squeeze(1)\n",
    "            loss = criterion(out, label)\n",
    "            acc = get_accuracy(out, label)\n",
    "            num_epoch +=1 \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss/num_epoch, epoch_acc/num_epoch          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(batch_data, model, criterion, optimizer, get_accuracy):\n",
    "    model.train()\n",
    "    num_epoch = epoch_loss = epoch_acc = 0\n",
    "    for text, label in batch_data:\n",
    "        model.zero_grad()\n",
    "        out = model(text).squeeze(1)\n",
    "        loss = criterion(out, label)\n",
    "        acc = get_accuracy(out, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        num_epoch +=1 \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "    \n",
    "    return epoch_loss/num_epoch, epoch_acc/num_epoch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 :\n",
      "\t Train Loss: 0.1558 | Train Acc: 94.39%\n",
      "\t Valid Loss: 0.6171 | Valid Acc: 82.25%\n",
      "Epoch: 02 :\n",
      "\t Train Loss: 0.1550 | Train Acc: 94.45%\n",
      "\t Valid Loss: 0.6319 | Valid Acc: 81.47%\n",
      "Epoch: 03 :\n",
      "\t Train Loss: 0.1526 | Train Acc: 94.53%\n",
      "\t Valid Loss: 0.6300 | Valid Acc: 82.59%\n",
      "Epoch: 04 :\n",
      "\t Train Loss: 0.1510 | Train Acc: 94.60%\n",
      "\t Valid Loss: 0.6502 | Valid Acc: 81.25%\n",
      "Epoch: 05 :\n",
      "\t Train Loss: 0.1495 | Train Acc: 94.64%\n",
      "\t Valid Loss: 0.6515 | Valid Acc: 82.37%\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCH = 30\n",
    "best_valid_acc = -1\n",
    "\n",
    "dev_data = generate_batch(num_dev_data)\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    train_data = generate_batch(num_train_data)\n",
    "    train_loss, train_acc = train(train_data, model, criterion, optimizer, get_accuracy)\n",
    "    valid_loss, valid_acc = evaluate(dev_data, model, criterion, get_accuracy)\n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(model.state_dict(),'avg-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} :')\n",
    "    print(f'\\t Train Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Valid Loss: {valid_loss:.4f} | Valid Acc: {valid_acc*100:.2f}%')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('avg-model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.5369 |  Test Acc: 81.23%\n"
     ]
    }
   ],
   "source": [
    "test_data = generate_batch(num_test_data)\n",
    "test_loss, test_acc = evaluate(test_data, model, criterion, get_accuracy)\n",
    "print(f'Test Loss: {test_loss:.4f} |  Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 打印词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embed = model.get_embed_weigth()\n",
    "embed_norm = torch.norm(embed, p=None, dim=1)\n",
    "sort_embed_norm, sort_embed_norm_idx = embed_norm.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm 最小的30个单词：\n",
      "par / holiday / pastiche / seedy / e-graveyard / quieter / home / captain / keeps / possibly / urge / aching / career / album / code / elegy / peculiar / squint / handheld / blown / quite / cops / miss / the / blush / judd / trip / appointed / make / themselves / "
     ]
    }
   ],
   "source": [
    "print('norm 最小的30个单词：')\n",
    "for idx in sort_embed_norm_idx[:30].tolist():\n",
    "    print(idx2word[idx], end=' / ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm 最大的30个单词：\n",
      "wonderfully / lousy / unlikable / choppy / badly / splendid / worst / dazzling / outstanding / inept / listless / lacking / playful / mesmerizing / unnecessary / amazing / stunning / irritating / unimaginative / refreshingly / heartwarming / devoid / riveting / suffers / tiresome / pointless / thought-provoking / poorly / mess / unfunny / "
     ]
    }
   ],
   "source": [
    "print('norm 最大的30个单词：')\n",
    "for idx in sort_embed_norm_idx[-30:].tolist():\n",
    "    print(idx2word[idx], end=' / ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "norm 最大的30个单词都是和电影评价相关的词语 \n",
    "\n",
    "norm 最小的30个单词 都是和对电影情感评价无关的词语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
